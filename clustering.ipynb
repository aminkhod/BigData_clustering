{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from NPIR import NPIR\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "import warnings\n",
    "from collections import Counter as Cs\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "########\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "########\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from operator import *\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.types import IntegerType, FloatType, BooleanType, StringType, StructType,\\\n",
    "StructField,ArrayType, DataType\n",
    "from pyspark.sql.functions import udf, log, rand, monotonically_increasing_id, col, broadcast,\\\n",
    "greatest, desc, asc, row_number, avg, mean, least, struct, lit, sequence, sum\n",
    "from functools import reduce\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, SQLContext, Window, Row, DataFrame\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.sql.broadcastTimeout\", \"30000s\").\\\n",
    "config(\"spark.network.timeout\",\"30000s\").config(\"spark.executor.heartbeatInterval\",\"12000000ms\").\\\n",
    "config(\"spark.storage.blockManagerSlaveTimeoutMs\",\"12000001ms\").config(\"spark.driver.maxResultSize\",\"12g\").\\\n",
    "config(\"spark.default.parallelism\", \"100\").config(\"spark.memory.offHeap.enabled\",\"true\").\\\n",
    "config(\"spark.memory.offHeap.size\", \"25g\").appName(\"NPIR_Parallel\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.785319</td>\n",
       "      <td>5.568516</td>\n",
       "      <td>-10.649756</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.565005</td>\n",
       "      <td>4.564475</td>\n",
       "      <td>-9.979172</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.497358</td>\n",
       "      <td>-5.706487</td>\n",
       "      <td>-8.941820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.507116</td>\n",
       "      <td>4.077689</td>\n",
       "      <td>-11.093956</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.918326</td>\n",
       "      <td>-1.266070</td>\n",
       "      <td>-1.457767</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1          2  label\n",
       "0 -1.785319  5.568516 -10.649756      0\n",
       "1 -2.565005  4.564475  -9.979172      0\n",
       "2 -4.497358 -5.706487  -8.941820      1\n",
       "3 -1.507116  4.077689 -11.093956      0\n",
       "4 -5.918326 -1.266070  -1.457767      2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1 = make_blobs(n_samples=1000, centers=3, n_features=3,\n",
    "                random_state=1)\n",
    "data = pd.DataFrame(x1)\n",
    "data['label'] = y1\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-6.178202</td>\n",
       "      <td>-2.290967</td>\n",
       "      <td>-0.949075</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>-5.823324</td>\n",
       "      <td>-2.282588</td>\n",
       "      <td>-1.217316</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>-7.821134</td>\n",
       "      <td>-4.202928</td>\n",
       "      <td>-1.475274</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>-4.981322</td>\n",
       "      <td>-6.513678</td>\n",
       "      <td>-8.113955</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>-4.924054</td>\n",
       "      <td>-7.057942</td>\n",
       "      <td>-7.374577</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2  label\n",
       "9   -6.178202 -2.290967 -0.949075      2\n",
       "578 -5.823324 -2.282588 -1.217316      2\n",
       "170 -7.821134 -4.202928 -1.475274      2\n",
       "594 -4.981322 -6.513678 -8.113955      1\n",
       "886 -4.924054 -7.057942 -7.374577      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = shuffle(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('blobs3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "data_spark_df = spark.read.format('csv').option('header','True').option('index','False').load('blobs3.csv')\n",
    "# data_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, 0: string, 1: string, 2: string, label: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_spark_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 0: string (nullable = true)\n",
      " |-- 1: string (nullable = true)\n",
      " |-- 2: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spark_df = data_spark_df.select(data_spark_df.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_name = ['first', 'second']\n",
    "# data_spark_rdd = data_spark_df.toDF(*new_name).rdd.filter(lambda x:x)\n",
    "# data_spark_df = data_spark_df.toDF(*new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "spark.conf.set('spark.jars.packages','com.databricks:spark-cav_2.11')\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"False\")\n",
    "sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1,y1 = make_blobs(n_samples=100, centers=3, n_features=2,\n",
    "#                 random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|                  0|                  1|                  2|\n",
      "+-------------------+-------------------+-------------------+\n",
      "| -6.178202088636509| -2.290966918339164|-0.9490748263927062|\n",
      "| -5.823323667985372|-2.2825881261168344| -1.217315876498287|\n",
      "| -7.821134075111614| -4.202928382013665|-1.4752743062790252|\n",
      "| -4.981322111046983| -6.513677821244844| -8.113955406506056|\n",
      "| -4.924054366659606| -7.057942122967516| -7.374577227824681|\n",
      "|-1.8402867927171944| -8.589592868288202| -7.261256119112246|\n",
      "|0.03226622162464632| 3.6585361273706116|-10.578509726909694|\n",
      "| -5.401953589245766| -5.461835726512175| -8.899617899703543|\n",
      "|  -2.85043320276864| -8.558624753018153| -7.359455196361065|\n",
      "| -5.900266005997015|-2.0334089913519104|-1.4132158222772127|\n",
      "| -6.902749810189315|  -3.81092111433266|-0.9798829886588849|\n",
      "|-7.5418494910146014| -3.685305979738968| -2.185262156411198|\n",
      "|0.07586625043811068| 3.2913682000334887|-10.128802254614401|\n",
      "| 1.9137991679036563|  3.889745439298058| -9.726712389779705|\n",
      "| -4.352181329068178|  -8.03825717211929| -8.339472012458868|\n",
      "| -0.847024717421322|  4.617391900250656| -9.575933349002614|\n",
      "|-2.1261646197758806|  4.496831052550813| -9.402977799482407|\n",
      "| -7.306720340398441|-3.4248272390310626|-1.9459339090616168|\n",
      "| -5.048913572993829| -3.200838552161368|  -1.74650376918101|\n",
      "| -2.074370060288439|  4.377973909968945| -8.463406390449112|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = data_spark_df.count()\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IR: The indexing ratio to be used for generating the maximum index\n",
    "IR = 0.2\n",
    "#The number of iteration i\n",
    "i = 10\n",
    "k = 3 #k: Number of clusters\n",
    "# count = Cs()\n",
    "chunk = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = timer()\n",
    "# kmeans = KMeans(n_clusters=k, random_state=0).fit(x)\n",
    "# end = timer()\n",
    "# print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data_spark = data_spark_df.withColumn('index_column_name', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "# data_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', 'index_column_name']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chunkLabel', 'old label', '1', '2', '3')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderheadr = ['chunkLabel', 'old label'\n",
    "               #, 'count'\n",
    "              ]\n",
    "leaderheadr.extend([str(x) for x in range(1, len(data_spark.columns))])\n",
    "leaderheadr = tuple(leaderheadr)\n",
    "leaderheadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chunkLabel', 'label')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelsheader = ('chunkLabel', 'label')\n",
    "labelsheader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time HH:MM:SS: 0:00:08.611831\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "\n",
    "labels = sqlContext.createDataFrame([np.full(len(labelsheader), np.nan).tolist()],labelsheader)\n",
    "labels = labels.na.drop()\n",
    "\n",
    "leaders = sqlContext.createDataFrame([np.full(len(leaderheadr), np.nan).tolist()],leaderheadr)\n",
    "leaders = leaders.na.drop()\n",
    "\n",
    "ii = 0\n",
    "for z in range(0, points, chunk):\n",
    "    j = z + chunk\n",
    "    data = data_spark.where(col(\"index_column_name\").between(z, j-1)).toPandas()\n",
    "    data.drop(\"index_column_name\",axis=1,inplace=True)\n",
    "    data = data.astype(float)\n",
    "    from NPIR import NPIR\n",
    "    label = NPIR(data.values,k,IR,i)\n",
    "    data['labels'] = label\n",
    "    \n",
    "    # Adding to pyspard label\n",
    "    chunklabel = np.full(len(label), ii).tolist()\n",
    "    labelDF = [(x, y) for x, y in zip(chunklabel, label)]\n",
    "    labelsDF = sqlContext.createDataFrame(labelDF,labelsheader)\n",
    "    labels = unionAll(labels, labelsDF)\n",
    "\n",
    "    leader = []\n",
    "    f = list(Cs(label))\n",
    "    f.sort()\n",
    "    for i in f:\n",
    "        leader.append([round(np.mean(z), 4) for z in data[data['labels']==i].values[:,:-1].T])\n",
    "    del data\n",
    "    del NPIR\n",
    "    \n",
    "    # Adding to pyspark leaders\n",
    "    for x in range(len(leader)):\n",
    "        x1 = [ii, x]\n",
    "        x1.extend(leader[x])\n",
    "        leader[x] = x1\n",
    "    leaderDF = sqlContext.createDataFrame(leader,leaderheadr)\n",
    "    leaders = unionAll(leaders, leaderDF)\n",
    "    ii += 1\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cs(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time HH:MM:SS: 0:00:24.632054\n"
     ]
    }
   ],
   "source": [
    "# # Load and parse the data\n",
    "# data = sc.textFile(\"data/mllib/kmeans_data.txt\")\n",
    "# parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "parsedData = leaders.select(leaders.columns[2:]).rdd.map(list)\n",
    "\n",
    "start = timer()\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, k, maxIterations=20, initializationMode=\"random\")\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\", timedelta(seconds = end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "# def error(point):\n",
    "#     center = clusters.centers[clusters.predict(point)]\n",
    "#     return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "# WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "# print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters.centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save and load model\n",
    "# clusters.save(sc, \"KMeansModel\")\n",
    "# sameModel = KMeansModel.load(sc, \"KMeansModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import array\n",
    "# from math import sqrt\n",
    "\n",
    "# from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "# # Load and parse the data\n",
    "# data = sc.textFile(\"data/mllib/kmeans_data.txt\")\n",
    "# parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "\n",
    "# # Build the model (cluster the data)\n",
    "# clusters = KMeans.train(parsedData, 2, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "# # Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "# def error(point):\n",
    "#     center = clusters.centers[clusters.predict(point)]\n",
    "#     return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "# WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "# print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "# # Save and load model\n",
    "# clusters.save(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")\n",
    "# sameModel = KMeansModel.load(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c1x c2y c3z\n",
    "# /\n",
    "# (c1 + c2 + c3)\n",
    "\n",
    "# sum = (c1 + c2 + c3)\n",
    "# leader[c1/sum*x +c2/sum*y+ c3/sum*z]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaders = leaders.select(['chunkLabel','old label','1','2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_spark = leaders.withColumn(\"first_numeric\", leaders[\"1\"].cast(FloatType()))\n",
    "# data_spark = data_spark.withColumn(\"second_numeric\", data_spark[\"2\"].cast(FloatType()))\n",
    "# data_spark = data_spark.drop('1').drop('2').drop('chunkLabel').drop('old label')\n",
    "# # #### Compute weighted average\n",
    "# # count = data_spark.select('count')\n",
    "# # ####\n",
    "\n",
    "# data_spark = data_spark.drop('1').drop('2').drop('chunkLabel').drop('old label').drop('count')\n",
    "# data_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = count.withColumn('Cindex', row_number().\\\n",
    "#                                           over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "# count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def MyCheckUpdate(a, b, c, d):\n",
    "#     a = float(a)\n",
    "#     b = float(b)\n",
    "#     c = float(c)\n",
    "#     d = float(d)\n",
    "#     res = (a-c) + (b-d)\n",
    "#     if res == 0:\n",
    "#         return 1.0\n",
    "#     return 0.0\n",
    "\n",
    "# check_centroid = udf(lambda x,y,z,r: MyCheckUpdate(x,y,z,r), FloatType())\n",
    "\n",
    "def squaree1(c,u,f,g):\n",
    "    c = float(c)\n",
    "    u = float(u)\n",
    "    f = float(f)\n",
    "    g = float(g)\n",
    "    array1 = np.array([c,u])\n",
    "    array2 = np.array([f,g])\n",
    "    dist = np.linalg.norm(array1-array2)\n",
    "    dist = dist.item()\n",
    "    return dist\n",
    "\n",
    "squaree_spark1 = udf(lambda x,y,z,r: squaree1(x,y,z,r), FloatType())\n",
    "sqlContext.sql(\"SET spark.sql.autoBroadcastJoinThreshold = -1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# c = -8\n",
    "# u = -9\n",
    "# f = -7\n",
    "# g = -8\n",
    "# array1 = np.array([c,u])\n",
    "# array2 = np.array([f,g])\n",
    "# dist = np.linalg.norm(array1-array2)\n",
    "# dist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_centroid = data_spark.sample(False, 0.4,seed = 0).limit(1).cache()\n",
    "\n",
    "# new_name = ['x','y']\n",
    "# df_centroid = df_centroid.toDF(*new_name)\n",
    "\n",
    "# #just for first round\n",
    "# #first time\n",
    "# i = 0\n",
    "# data_cent = data_spark.join(broadcast(df_centroid))\n",
    "# data_cent1 = data_cent.withColumn(str(i),squaree_spark1(data_cent.columns[0],data_cent.columns[1],\n",
    "#                                               data_cent.columns[2*i+2],data_cent.columns[2*i+3]))\n",
    "# data_cent2 = data_cent1.drop(data_cent1.columns[i+2]).drop(data_cent1.columns[i+3])\n",
    "# data_cent3 = data_cent2.withColumn('mindist',col(str(i)))\n",
    "# data_cent4 = data_cent3.withColumn('mindist1',least(data_cent3.columns[i+2], col('mindist')))\n",
    "# data_cent4 = data_cent4.drop('mindist')\n",
    "# data_cent5 = data_cent4.withColumnRenamed('mindist1','mindist')\n",
    "# next_selected = data_cent5.orderBy(desc('mindist')).limit(1).select(data_cent5.columns[0:2])#1:3\n",
    "\n",
    "# df_centroid = df_centroid.union(next_selected)\n",
    "# u = [str(i)+'x',str(i)+'y']\n",
    "# next_selected = next_selected.toDF(*u)\n",
    "# def initial_centroids(next_selected,data_cent_5_persist, i):\n",
    "#     data_cent6 = data_cent_5_persist.join(broadcast(next_selected))\n",
    "#     data_cent6 = data_cent6.withColumn(str(i),squaree_spark1(data_cent6.columns[0],data_cent6.columns[1],\n",
    "#                                              data_cent6.columns[i+3],data_cent6.columns[i+4]))#+4 +5\n",
    "#     data_cent6 = data_cent6.drop(data_cent6.columns[i+3]).drop(data_cent6.columns[i+4])#+4 +5\n",
    "#     data_cent6 = data_cent6.withColumn('mindist1',least(data_cent6.columns[i+3], col('mindist')))#4\n",
    "#     data_cent6 = data_cent6.drop('mindist')\n",
    "#     data_cent6 = data_cent6.withColumnRenamed('mindist1','mindist')\n",
    "#     next_cent = data_cent6.orderBy(desc('mindist')).limit(1).select(data_cent6.columns[0:2])#1:3\n",
    "#     return next_cent,data_cent6\n",
    "\n",
    "\n",
    "# data_cent_5_persist = data_cent5.persist(StorageLevel.MEMORY_ONLY_2)\n",
    "\n",
    "\n",
    "# # start = timer()\n",
    "# for i in range(1,k-1):\n",
    "#     next_selected, data_cent_5_persist = initial_centroids(next_selected,data_cent_5_persist, i)\n",
    "#     u = [str(i)+'x',str(i)+'y']\n",
    "#     next_selected = next_selected.toDF(*u)\n",
    "\n",
    "# # end = timer()\n",
    "# # print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))\n",
    "\n",
    "\n",
    "\n",
    "# i= k-1\n",
    "# data_cent11 = data_cent_5_persist.join(broadcast(next_selected))\n",
    "# data_cent11 = data_cent11.withColumn(str(i),squaree_spark1(data_cent11.columns[0],data_cent11.columns[1],\\\n",
    "#                                                            data_cent11.columns[k+2],data_cent11.columns[k+3]))\n",
    "# data_cent11 = data_cent11.drop('mindist').drop(data_cent11.columns[k+2]).drop(data_cent11.columns[k+3])\n",
    "\n",
    "\n",
    "# def FindMinCOl( *row_list):\n",
    "#     ind = row_list.index(min(*row_list))\n",
    "#     return int(ind)\n",
    "\n",
    "# find_min_val_name = udf(FindMinCOl, IntegerType())\n",
    "# data_cent14 = data_cent11.withColumn('defined_cluster', find_min_val_name(*data_cent11.columns[2:3+k]))\n",
    "# data_cent14 = data_cent14.select('first_numeric','second_numeric','defined_cluster')\n",
    "\n",
    "\n",
    "# # #### Compute weighted average\n",
    "# # data_cent14 = data_cent14.withColumn('index', row_number().\\\n",
    "# #                                           over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "# # data_cent14 = (data_cent14.join(count, (col('Cindex') == col('index')),\\\n",
    "# #           \"inner\")).drop('index').drop('Cindex')\n",
    "\n",
    "# # for c in data_spark.columns:\n",
    "# #     data_cent14 = data_cent14.withColumn(c+'*',col(c)*col('count') )\n",
    "# #     data_cent14 = data_cent14.drop(c).withColumnRenamed(c+'*', c)\n",
    "\n",
    "# # new_centroid = data_cent14.groupBy('defined_cluster').sum('first_numeric', 'second_numeric').\\\n",
    "# # withColumnRenamed('defined_cluster', 'defined_cluster*')\n",
    "\n",
    "# # new_centroid1 = data_cent14.groupBy('defined_cluster').sum('count')\n",
    "# # new_centroid = (new_centroid1.join(new_centroid, (col('defined_cluster') == col('defined_cluster*')),\\\n",
    "# #           \"inner\")).drop('defined_cluster*')\n",
    "# # ####\n",
    "# # for c in new_centroid.columns:\n",
    "# #     new_centroid = new_centroid.withColumnRenamed(c, c.replace('sum(', '').replace(')', ''))\n",
    "\n",
    "# # for c in new_centroid.columns[2:]:\n",
    "# #     new_centroid = new_centroid.withColumn(c, col(c) / col('sum(count)'))\n",
    "\n",
    "# # new_centroid = new_centroid.drop('sum(count)')\n",
    "# # ####\n",
    "# new_centroid = data_cent14.groupBy('defined_cluster').avg('first_numeric', 'second_numeric')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spark.sparkContext.getConf().getAll()\n",
    "# spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "# def UpdateCentroid(x):\n",
    "#     data_cent_join1 = data_spark.join(broadcast(x))\n",
    "#     data_cent_join2 = data_cent_join1.withColumn('dist',squaree_spark1(data_cent_join1.columns[0],\n",
    "#                                                                  data_cent_join1.columns[1],\n",
    "#                                        data_cent_join1.columns[3],data_cent_join1.columns[4]))#3 4\n",
    "#     w = Window.partitionBy(data_cent_join2.columns[1])\n",
    "#     next_centroid = data_cent_join2.withColumn('mindist', F.min('dist').over(w)).\\\n",
    "#     filter(col('dist') == col('mindist')).drop('dist')\n",
    "\n",
    "# #     #### Compute weighted average\n",
    "# #     data_cent14 = next_centroid.withColumn('index', row_number().\\\n",
    "# #                                               over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "# #     data_cent14 = (data_cent14.join(count, (col('Cindex') == col('index')),\\\n",
    "# #               \"inner\")).drop('index').drop('Cindex')\n",
    "\n",
    "# #     for c in data_spark.columns:\n",
    "# #         data_cent14 = data_cent14.withColumn(c+'*',col(c)*col('count') )\n",
    "# #         data_cent14 = data_cent14.drop(c).withColumnRenamed(c+'*', c)\n",
    "\n",
    "# #     new_centroid = data_cent14.groupBy('defined_cluster').sum('first_numeric', 'second_numeric').\\\n",
    "# #     withColumnRenamed('defined_cluster', 'defined_cluster*')\n",
    "\n",
    "# #     new_centroid1 = data_cent14.groupBy('defined_cluster').sum('count')\n",
    "# #     new_centroid = (new_centroid1.join(new_centroid, (col('defined_cluster') == col('defined_cluster*')),\\\n",
    "# #               \"inner\")).drop('defined_cluster*')\n",
    "\n",
    "\n",
    "# #     for c in new_centroid.columns[2:]:\n",
    "# #         new_centroid = new_centroid.withColumn(c, col(c) / col('sum(count)'))\n",
    "\n",
    "# #     update_new_centroid = new_centroid.drop('sum(count)')\n",
    "# #     ############\n",
    "#     update_new_centroid = next_centroid.groupBy('defined_cluster').avg('first_numeric', 'second_numeric')\n",
    "\n",
    "#     return update_new_centroid, next_centroid\n",
    "\n",
    "# new_centroid_persist = new_centroid.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# # start = timer()\n",
    "\n",
    "# for i in range(20):\n",
    "#     new_centroid_persist, final_data = UpdateCentroid(new_centroid_persist)\n",
    "\n",
    "# # end = timer()\n",
    "\n",
    "\n",
    "# final_data = final_data.withColumnRenamed('avg(first_numeric)','cent_x').\\\n",
    "# withColumnRenamed('avg(second_numeric)','cent_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# new_centroid_persist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(new_centroid_persist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_cent11 = sc.parallelize([])\n",
    "start = timer()\n",
    "\n",
    "spark_cluster_centroid = sqlContext.createDataFrame(([c.tolist() for c in clusters.centers]),\\\n",
    "                                                    ['cent_x', 'cent_y'])\n",
    "spark_cluster_centroid = spark_cluster_centroid.withColumn('defined_cluster', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "data_cent11 = 0\n",
    "for i in range(k):\n",
    "    u = [ str(i)+'x',str(i)+'y']\n",
    "    next_selected = spark_cluster_centroid.filter(col('defined_cluster') == str(i)).\\\n",
    "    drop('defined_cluster').toDF(*u)\n",
    "    if i == 0:\n",
    "        data_cent11 = data_spark_df.join(broadcast(next_selected))\n",
    "#         print(data_cent.count())\n",
    "        \n",
    "        data_cent11 = data_cent11.withColumn(str(i),squaree_spark1(data_cent11.columns[0],\\\n",
    "                            data_cent11.columns[1],data_cent11.columns[i+2],data_cent11.columns[i+3]))\n",
    "#         data_cent11.show()\n",
    "        data_cent11 = data_cent11.drop(data_cent11.columns[i+2]).drop(data_cent11.columns[i+3])\n",
    "#         data_cent11.show()\n",
    "        data_cent11 = data_cent11.withColumn('mindist',col(str(i)))\n",
    "#         data_cent11.show()\n",
    "        data_cent11 = data_cent11.withColumn('mindist1',least(data_cent11.columns[i+2], col('mindist')))\n",
    "#         data_cent11.show()\n",
    "        data_cent11 = data_cent11.drop('mindist')\n",
    "        data_cent11 = data_cent11.withColumnRenamed('mindist1','mindist')\n",
    "    elif i > 0:\n",
    "        data_cent11 = data_cent11.join(broadcast(next_selected))\n",
    "#         data_cent11.show()\n",
    "        data_cent11 = data_cent11.withColumn(str(i),squaree_spark1(data_cent11.columns[0],\\\n",
    "                                    data_cent11.columns[1], data_cent11.columns[i+3],data_cent11.columns[i+4]))\n",
    "#         data_cent11.show()\n",
    "        data_cent11 = data_cent11.drop(u[0]).drop(u[1])\n",
    "#         data_cent11.show()\n",
    "        data_cent11 = data_cent11.withColumn('mindist1',least(data_cent11.columns[i+3], col('mindist')))#4\n",
    "#         data_cent11.show()\n",
    "        data_cent11 = data_cent11.drop('mindist')\n",
    "        data_cent11 = data_cent11.withColumnRenamed('mindist1','mindist')\n",
    "#         next_cent = data_cent11.orderBy(desc('mindist')).limit(1).select(data_cent11.\\\n",
    "#                                                                          columns[:len(data_spark_df.columns)-1])#1:3\n",
    "data_cent11 = data_cent11.drop('mindist')\n",
    "\n",
    "# data_cent11.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindMinCOl( *row_list):\n",
    "    ind = row_list.index(min(*row_list))\n",
    "    return int(ind)\n",
    "find_min_val_name = udf(FindMinCOl, IntegerType())\n",
    "\n",
    "data_cent11 = data_cent11.withColumn('defined_cluster', find_min_val_name(*data_cent11.columns[2:3+k]))\n",
    "data_cent11 = data_cent11.select('first','second','defined_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data_cent11.toPandas()\n",
    "\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The first row in RDD is empty, can not infer schema",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-4bb0cdb515cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \"\"\"\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    603\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    604\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \"\"\"\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             raise ValueError(\"The first row in RDD is empty, \"\n\u001b[0m\u001b[1;32m    399\u001b[0m                              \"can not infer schema\")\n\u001b[1;32m    400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The first row in RDD is empty, can not infer schema"
     ]
    }
   ],
   "source": [
    "clusters.predict(parsedData).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "col should be Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-446c00795a32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspark_cluster_centroid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsedData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark_cluster_centroid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_cluster_centroid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'defined_cluster'\u001b[0m\u001b[0;34m,\u001b[0m                                                           \u001b[0mclusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mspark_cluster_centroid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2094\u001b[0m         \"\"\"\n\u001b[0;32m-> 2095\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2096\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: col should be Column"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "spark_cluster_centroid = parsedData.toDF()\n",
    "spark_cluster_centroid = spark_cluster_centroid.withColumn('defined_cluster',\\\n",
    "                                                           clusters.predict(parsedData))\n",
    "spark_cluster_centroid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data = new_centroid_persist.withColumnRenamed('avg(first_numeric)','cent_x').\\\n",
    "# withColumnRenamed('avg(second_numeric)','cent_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-------------------+---------------+-----+\n",
      "|                 _1|      _2|                 _3|defined_cluster|index|\n",
      "+-------------------+--------+-------------------+---------------+-----+\n",
      "| -4.085240000000001|  -6.905|           -8.08314|              0|    0|\n",
      "|            -6.2483|-3.13736|-2.0244600000000004|              1|    1|\n",
      "|-1.6059199999999998| 4.39784|           -9.90502|              2|    2|\n",
      "+-------------------+--------+-------------------+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = spark_cluster_centroid.withColumn('index', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "# .\\\n",
    "# drop('mindist').drop('second_numeric').drop('first_numeric')\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+-------+-------+-----------------+\n",
      "|chunkLabel|old label|      1|      2|      3|index_column_name|\n",
      "+----------+---------+-------+-------+-------+-----------------+\n",
      "|       0.0|      0.0|-3.9357|-6.9772|-8.0228|                0|\n",
      "|       0.0|      1.0|-6.2496|-3.0875|-1.8382|                1|\n",
      "|       0.0|      2.0| -1.339| 4.4668|-9.9646|                2|\n",
      "|       1.0|      0.0|-6.2928|-3.2643| -1.894|                3|\n",
      "|       1.0|      1.0|-4.0941|-6.7347|-7.9202|                4|\n",
      "|       1.0|      2.0|-1.6081| 4.4068|-9.9675|                5|\n",
      "|       2.0|      0.0| -6.355| -2.988|-2.1373|                6|\n",
      "|       2.0|      1.0|-4.1492|-7.1096|-8.2813|                7|\n",
      "|       2.0|      2.0|-1.6843| 4.2302|-9.8328|                8|\n",
      "|       3.0|      0.0|-4.0884|-6.7994|-8.2291|                9|\n",
      "|       3.0|      1.0|-1.5726| 4.5908|-9.9561|               10|\n",
      "|       3.0|      2.0| -6.306|-3.2471|-2.1483|               11|\n",
      "|       4.0|      0.0|-1.8256| 4.2946|-9.8041|               12|\n",
      "|       4.0|      1.0|-4.1588|-6.9041|-7.9623|               13|\n",
      "|       4.0|      2.0|-6.0381|-3.0999|-2.1045|               14|\n",
      "+----------+---------+-------+-------+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leaders1 = leaders.withColumn('index_column_name', row_number().\\\n",
    "                    over(Window.orderBy(monotonically_increasing_id())) - 1).drop('count')\n",
    "leaders1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaders1 = leaders.drop('count')\n",
    "# leaders1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+-------+-------+-------------------+--------+-------------------+---------------+\n",
      "|chunkLabel|old label|      1|      2|      3|                 _1|      _2|                 _3|defined_cluster|\n",
      "+----------+---------+-------+-------+-------+-------------------+--------+-------------------+---------------+\n",
      "|       0.0|      0.0|-3.9357|-6.9772|-8.0228| -4.085240000000001|  -6.905|           -8.08314|              0|\n",
      "|       0.0|      1.0|-6.2496|-3.0875|-1.8382|            -6.2483|-3.13736|-2.0244600000000004|              1|\n",
      "|       0.0|      2.0| -1.339| 4.4668|-9.9646|-1.6059199999999998| 4.39784|           -9.90502|              2|\n",
      "+----------+---------+-------+-------+-------+-------------------+--------+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lead_final = (leaders1\n",
    "    .join(final_data, (col('index_column_name') == col('index')), \"inner\")).\\\n",
    "drop('index_column_name').drop('index')\n",
    "#     .withColumnRenamed(\"defined_cluster\", \"new label\"))\n",
    "lead_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------------+--------+-------------------+---------------+\n",
      "|chunkLabel|old label|                 _1|      _2|                 _3|defined_cluster|\n",
      "+----------+---------+-------------------+--------+-------------------+---------------+\n",
      "|       0.0|      0.0| -4.085240000000001|  -6.905|           -8.08314|              0|\n",
      "|       0.0|      1.0|            -6.2483|-3.13736|-2.0244600000000004|              1|\n",
      "|       0.0|      2.0|-1.6059199999999998| 4.39784|           -9.90502|              2|\n",
      "+----------+---------+-------------------+--------+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in leaders1.columns[2:-1]:\n",
    "    lead_final = lead_final.drop(col)\n",
    "lead_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_final = lead_final.withColumnRenamed('chunkLabel','chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-6c6629d4c7f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m label_lead_final = (labels\n\u001b[0;32m----> 2\u001b[0;31m     .join(lead_final,  ((col('label') == col('old label')) & (col('chunkLabel') == col('chunk'))),\\\n\u001b[0m\u001b[1;32m      3\u001b[0m           \"left\")).drop('chunkLabel').drop('label').drop('old label').drop('chunk')\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabel_lead_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "label_lead_final = (labels\n",
    "    .join(lead_final,  ((col('label') == col('old label')) & (col('chunkLabel') == col('chunk'))),\\\n",
    "          \"left\")).drop('chunkLabel').drop('label').drop('old label').drop('chunk')\n",
    "\n",
    "label_lead_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = label_lead_final.toPandas()\n",
    "# d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primariy Data: Max( o(n \\* p), o(n \\* n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# means:  uper(n / chunks) \\* k \\* p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 * 1) > 1 / chunks * k * 1 => chunks > k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# labels: n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_spark_df.where(col(\"index_column_name\").between(0, chunk-1)).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = list(range(data_spark_df.count()))\n",
    "# # data_spark_df.withColumn('index', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = timer()\n",
    "\n",
    "# labelsPred = NPIR(x,k,IR,i)\n",
    "# # labelsPred1 = NPIR(x1,k,IR,i)\n",
    "# end = timer()\n",
    "# print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))\n",
    "# labelsPred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xf = pd.DataFrame(x)\n",
    "# xf['label'] = labelsPred\n",
    "\n",
    "# xf1 = pd.DataFrame(x1)\n",
    "# xf1['label'] = labelsPred1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cs(labelsPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leader = []\n",
    "# f = list(Cs(labelsPred))\n",
    "# f.sort()\n",
    "# for i in f:\n",
    "#     leader.append([np.mean(z) for z in xf[xf['label']==i].values[:,:-1].T])\n",
    "# leader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leader1 = []\n",
    "# f = list(Cs(labelsPred1))\n",
    "# f.sort()\n",
    "# for i in f:\n",
    "#     leader1.append([np.mean(z) for z in xf1[xf1['label']==i].values[:,:-2].T])\n",
    "# leader1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xf1[xf1['label']==0].values[:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 2 3 4 5 6 7 \n",
    "# 1 2 3 4\n",
    "# 1 2\n",
    "# 1\n",
    "\n",
    "# 2**b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a \\* t(NPIR(n / a)) + (a - 1) \\* t(NPIR(k)) ><? t(NPIR(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centers = leader.copy()\n",
    "# centers.extend(leader1)\n",
    "# centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newlabel = NPIR(centers,k,IR,i)\n",
    "# newlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newlabel[:k][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newleader =[]\n",
    "# for i in range(k):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xf['newlabel'] = np.full(len(xf), np.nan)\n",
    "# xf1['newlabel'] =np.full(len(xf1), np.nan)\n",
    "# # f = list(Cs(labelsPred))\n",
    "# # f.sort()\n",
    "# for i in range(k):\n",
    "#     xf.loc[xf['label'] == i,'newlabel'] = newlabel[:k][i]\n",
    "\n",
    "# # f = list(Cs(labelsPred1))\n",
    "# # f.sort()\n",
    "# for i in range(k):\n",
    "#     xf1.loc[xf1['label'] == i,'newlabel'] = newlabel[-k:][i]\n",
    "# merged_data = xf.append(xf1)\n",
    "# merged_data['label'] = merged_data['newlabel']\n",
    "# merged_data.drop(['newlabel'], inplace =True,axis=1)\n",
    "# merged_data.reset_index(inplace=True)\n",
    "# merged_data.drop(['index'], inplace =True,axis=1)\n",
    "# merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xf = merged_data.copy()\n",
    "# labelsPred = merged_data['label']\n",
    "# xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tStart = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('blobs2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['0','1','label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = d1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['new'] = d['defined_cluster'].astype('int8')\n",
    "# data['new'] = NPIR(pd.read_csv('blobs.csv', header=None).iloc[:,:2].values,k,IR,i)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['label']\n",
    "labelsPred = data['new']\n",
    "# list(labelsPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[['0','1']].values\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing results\n",
    "print('labels:')\n",
    "# print(labelsPred)\n",
    "\n",
    "# tEnd = datetime.datetime.now()\n",
    "# print('Time: ' + str(tEnd - tStart))\n",
    "print('Measures:')\n",
    "print('HS: ' + str(metrics.homogeneity_score(y,labelsPred)))\n",
    "print('CS: ' + str(metrics.completeness_score(y,labelsPred)))\n",
    "print('VM: ' + str(metrics.v_measure_score(y,labelsPred)))\n",
    "print('AMI: ' + str(metrics.adjusted_mutual_info_score(y,labelsPred)))\n",
    "print('ARI: ' + str(metrics.adjusted_rand_score(y,labelsPred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle, islice \n",
    "\n",
    "fig = plt.figure()      \n",
    "colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a','#f781bf', '#a65628', '#984ea3',\n",
    "                                    '#999999', '#e41a1c', '#dede00']),int(k))))\n",
    "plt.scatter(x[:, 0], x[:, 1], s=10, color=colors[labelsPred.tolist()])\n",
    "plt.show()\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from itertools import cycle, islice \n",
    "\n",
    "# fig = plt.figure()      \n",
    "# colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a','#f781bf', '#a65628', '#984ea3',\n",
    "#                                     '#999999', '#e41a1c', '#dede00']),int(k))))\n",
    "# plt.scatter(x1[:, 0], x1[:, 1], s=10, color=colors[labelsPred1.tolist()])\n",
    "# plt.show()\n",
    "# # plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from itertools import cycle, islice \n",
    "\n",
    "# fig = plt.figure()      \n",
    "# colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a','#f781bf', '#a65628', '#984ea3',\n",
    "#                                     '#999999', '#e41a1c', '#dede00']),int(k))))\n",
    "# plt.scatter(merged_data.iloc[:, 0].values, merged_data.iloc[:, 1].values, s=10, color=colors[merged_data['label'].astype(int).tolist()])\n",
    "# plt.show()\n",
    "# # plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
