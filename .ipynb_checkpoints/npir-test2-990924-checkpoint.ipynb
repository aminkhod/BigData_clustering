{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count point:  10\n",
      "points:  [(2, 4), (4, 5), (3, 8), (6, 7), (1, 5), (6, 2), (3, 7), (8, 9), (5, 8), (3, 6)]\n",
      "+---------+\n",
      "|idx_point|\n",
      "+---------+\n",
      "|        0|\n",
      "|        1|\n",
      "|        2|\n",
      "|        3|\n",
      "|        4|\n",
      "|        5|\n",
      "|        6|\n",
      "|        7|\n",
      "|        8|\n",
      "|        9|\n",
      "+---------+\n",
      "\n",
      "+---------+-------------+\n",
      "|idx_point|ind_nxt_point|\n",
      "+---------+-------------+\n",
      "|        0|            0|\n",
      "|        1|            0|\n",
      "|        2|            0|\n",
      "|        3|            0|\n",
      "|        4|            0|\n",
      "|        5|            0|\n",
      "|        6|            0|\n",
      "|        7|            0|\n",
      "|        8|            0|\n",
      "|        9|            0|\n",
      "+---------+-------------+\n",
      "\n",
      "initial_Point:  [0, 3, 5]\n",
      "ls_clu_Pnt:  [[0, -1], [1], [2], [3, -1], [4], [5, -1], [6], [7], [8], [9]]\n",
      "ls_sel_pnt:  [0, 3, 5]\n",
      "*****************start clustering********************\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "# Big Data Programming \n",
    "# Lab 2 - Spark \n",
    "\n",
    "\n",
    "import math\n",
    "from scipy.spatial import distance\n",
    "from math import sqrt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "#from pyspark.sql.functions import lit\n",
    "#from pyspark.sql.functions import min\n",
    "\n",
    "from pyspark.sql import Row\n",
    "#from pyspark.sql.types import ArrayType, IntegerType\n",
    "from pyspark.sql.functions import udf, col , min\n",
    "\n",
    "from scipy import spatial\n",
    "from treelib import Tree\n",
    "import random\n",
    "\n",
    "\n",
    "# ---------- helper functions ----------    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# ----------------- Main Program ---------------\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"NBA-kmeans\").getOrCreate()\n",
    "#sc=spark.sparkContext\n",
    "\n",
    "#--error timeout\n",
    "import pyspark as spark\n",
    "#conf = spark.SparkConf().setAppName(\"applicaiton\").set(\"spark.executor.heartbeatInterval\", \"200000\").set(\"spark.network.timeout\", \"300000\")\n",
    "#sc = spark.SparkContext.getOrCreate(conf)\n",
    "#sqlcontext = spark.SQLContext(sc)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SQL_DataFrame\")\\\n",
    "  .master(\"local\")\\\n",
    "  .config(\"spark.network.timeout\", \"90000\")\\\n",
    "  .config(\"spark.executor.heartbeatInterval\", \"80000\")\\\n",
    "  .getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "\n",
    "#import pyspark as spark\n",
    "#conf = spark.SparkConf().setMaster(\"yarn-client\").setAppName(\"sparK-mer\")\n",
    "#conf.set(\"spark.executor.heartbeatInterval\",\"3600s\")\n",
    "#conf.set(\"spark.executor.heartbeatInterval\",\"120s\")\n",
    "#sc = spark.SparkContext('local[2]', '', conf=conf) # uses 4 cores on your local machine\n",
    "#---\n",
    "# ---- load and preprocessing -------\n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"./shot_logs.csv\", header = \"true\", inferSchema = \"true\")\n",
    "##dataPts = df.filter(df.player_name == 'james harden').select('SHOT_DIST','CLOSE_DEF_DIST').na.drop()\n",
    "##dataRDD = dataPts.rdd.map(lambda r: (r[0], r[1], r[2]))\n",
    "\n",
    "#*************** count point \n",
    "#dataPts = df.filter(df.player_name == 'james harden').select('SHOT_DIST','CLOSE_DEF_DIST').na.drop()#count points:1054 point\n",
    "#dataPts = df.select('SHOT_DIST','CLOSE_DEF_DIST').na.drop() #count points: 128069 point\n",
    "#coords = dataPts.rdd.map(lambda r: (r[0], r[1])).collect()  \n",
    "coords = [(2, 4),(4, 5), (3,8), (6,7), (1,5), (6,2), (3,7), (8,9), (5,8), (3,6)]\n",
    "#***************\n",
    "\n",
    "#dataRDD = dataPts.rdd.map(lambda r: (r[0], r[1]))\n",
    "##print(dataRDD)\n",
    "#print(dataRDD.take(dataRDD.count()))\n",
    "#print(dataRDD.take(5))\n",
    "\n",
    "\n",
    "dataRDD = sc.parallelize(coords)\n",
    "#count_point=len(coords)\n",
    "\n",
    "#rdd = rdd.map(lambda x: [float(i) for i in x])\n",
    "#print(dataRDD.take(1))\n",
    "count_point=len(dataRDD.take(dataRDD.count()))\n",
    "print(\"count point: \",len(dataRDD.take(dataRDD.count())))\n",
    "print(\"points: \",dataRDD.take(dataRDD.count())) #all \n",
    "# -------- initialization ---------\n",
    "\n",
    "\n",
    "rdd_length = dataRDD.map(lambda x: len(x))\n",
    "rdd_length.collect()\n",
    "#print(rdd_length.take(rdd_length.count()))\n",
    "len_vec=rdd_length.take(1)[0] #len demention(features)\n",
    "#count_point=len(coords)\n",
    "#print(len_vec)\n",
    "\n",
    "\n",
    "totalindex=0\n",
    "ir=0.2\n",
    "index=0\n",
    "iters = 0 \n",
    "#old_centroid = int_centroid\n",
    "#old_centroid = coords\n",
    "k=3\n",
    "run=1\n",
    "\n",
    "#list_index_nearset=list(range(0, count_point) )\n",
    "list_index_nearset=[0]*count_point\n",
    "#print(\"list_index_nearset\",list_index_nearset)\n",
    "\n",
    "for m in range(run):\n",
    "\n",
    "    \n",
    "    schema = StructType([StructField('id', IntegerType(), True)])\n",
    "    df = spark.createDataFrame(spark.sparkContext.emptyRDD(),schema)\n",
    "    df = spark.range(0, count_point).withColumnRenamed(\"id\", \"idx_point\")\n",
    "    df.show()\n",
    "\n",
    "\n",
    "    df= df.withColumnRenamed('_1','idx_point')\n",
    "\n",
    "    #df8 = df.rdd.map(lambda x: [r for r in row(x)]).collect() \n",
    "    #print(df8)\n",
    "    \n",
    "    #from pyspark.ml.feature import VectorAssembler\n",
    "    #FEATURES_COL = df.columns[1:]\n",
    "    ##print(FEATURES_COL)\n",
    "    #vecAssembler = VectorAssembler(inputCols=FEATURES_COL, outputCol=\"features\")\n",
    "    #df = vecAssembler.transform(df).select('idx_point', \"features\")\n",
    "    df= df.withColumn('ind_nxt_point', F.lit(0))\n",
    "\n",
    "\n",
    "    df.show()\n",
    "\n",
    "    #def make_list(col):\n",
    "        #return list(map(int,[x for x in range(col+1) if x % 2 == 0]))    \n",
    "    #make_list_udf = udf(make_list, ArrayType(IntegerType()))\n",
    "    #df2 = df.withColumn('features',make_list_udf(col('features')))    \n",
    "    \n",
    "    ls_clu_Pnt=[] #list E:N,N,N..\n",
    "    ls_sel_pnt=[] #initial points & clustred points\n",
    "    ls_clu_Pnt = df.select('idx_point').rdd.flatMap(lambda x: [x]).collect()  #list E:N,N,N..\n",
    "    ls_clu_Pnt= [ele.__getattr__('idx_point') for ele in ls_clu_Pnt] #convert row to one list\n",
    "    ls_clu_Pnt= [ls_clu_Pnt[i:i+1] for i in range(0, len(ls_clu_Pnt), 1)] #convert list 1d to list 2d\n",
    "    #print('ls_clu_Pnt: ',ls_clu_Pnt.__str__()) # all element print\n",
    "\n",
    "    initial_Point = list(map(lambda row: row[0], df.select('idx_point').rdd.takeSample(False, 3))) #convert row out to one number\n",
    "    print('initial_Point: ',initial_Point)\n",
    "    #points_in_cluster = initial_Point\n",
    "    #ls_sel_pnt= initial_Point #list: all selected points for clustering(selected point in all clutser)\n",
    "    ls_sel_pnt.append(initial_Point[0])\n",
    "    ls_sel_pnt.append(initial_Point[1])\n",
    "    ls_sel_pnt.append(initial_Point[2])\n",
    "    \n",
    "    ls_clu_Pnt[ls_sel_pnt[0]].append(-1)\n",
    "    ls_clu_Pnt[ls_sel_pnt[1]].append(-1)\n",
    "    ls_clu_Pnt[ls_sel_pnt[2]].append(-1)\n",
    "    print('ls_clu_Pnt: ',ls_clu_Pnt.__str__()) # all element print\n",
    "    print('ls_sel_pnt: ',ls_sel_pnt)\n",
    "    #df = spark.createDataFrame(points_in_cluster, IntegerType()) #convert list to df\n",
    "    #df2 = spark.createDataFrame([(\"Range 1\", list([101,105])), (\"Range 2\", list([200, 203]))])\n",
    "\n",
    "    #----------------part 2-----------\n",
    "    print(\"*****************start clustering********************\")\n",
    "          \n",
    "    \n",
    "\n",
    "    \n",
    "    tree = spatial.cKDTree(coords,leafsize=16)\n",
    "    kdt_b = sc.broadcast(tree)\n",
    "    #distance,index = kdt_b.value.query([2.4, 4], k=2)\n",
    "    #print(distance,index)    \n",
    "    #print(index[1])    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------index:  1\n",
      "-------------------------------------------index:  2\n",
      "-------------------------------------------index:  3\n",
      "-------------------------------------------index:  4\n",
      "-------------------------------------------index:  5\n",
      "-------------------------------------------index:  6\n",
      "-------------------------------------------index:  7\n",
      "-------------------------------------------index:  8\n",
      "-------------------------------------------index:  9\n",
      "-------------------------------------------index:  10\n",
      "-------------------------------------------index:  11\n",
      "-------------------------------------------index:  12\n",
      "-------------------------------------------index:  13\n",
      "-------------------------------------------index:  14\n",
      "-------------------------------------------index:  15\n",
      "-------------------------------------------index:  16\n",
      "-------------------------------------------index:  17\n",
      "-------------------------------------------index:  18\n",
      "-------------------------------------------index:  19\n",
      "-------------------------------------------index:  20\n",
      "list_all_full_cluster: [[2, 6, 8, 3], [4, 0], [5, 1, 9]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#file 2---990924\n",
    "def deep_index(lst, w , frs_sec_idx, end_idx): #find num index in cluster(ls_clu_Pnt[1:] ,for two father)\n",
    "    return [(i, sub.index(w)) for (i, sub) in enumerate(lst) if w in sub[frs_sec_idx:end_idx]] #[1:] =>num elemnt of witch cluster?\n",
    "#def deep_index2(lst, w): #find num index in cluster(ls_clu_Pnt[1:] ,for two father)\n",
    "#    return [(i, sub.index(w)) for (i, sub) in enumerate(lst) if w in sub[0:1]] \n",
    "\n",
    "\n",
    "def clu_update(vec,num1):\n",
    "    tmp_point= vec[0]\n",
    "    if tmp_point==elec_point and (num1 not in vec):\n",
    "        vec.append(num1)       \n",
    "    #if (num1 in ls_sel_pnt):\n",
    "        #l=[[0], [1], [2], [3, 8], [4.0], [5], [6], [7, 3, 8], [8, 4], [9]]\n",
    "        ##print('l: ',l.__str__())\n",
    "        #Asign_point= deep_index(l, 3)\n",
    "        #Asign_point= deep_index(ls_clu_Pnt, num1) #find num index in cluster(ls_clu_Pnt[1:] ,for two father)\n",
    "        ##print(Asign_point) =>[(7, 1)] (1->index 3)\n",
    "        #Asign_point=Asign_point[0][0]\n",
    "        ##print(Asign_point) => 7 (7 -> cluster dad(Asign))\n",
    "\n",
    "        #user_func = udf (lambda x,y: [i for i, e in enumerate(x) if e==y ]) #find num index in list in column features\n",
    "        #df_idx_dad_min_dis = df.filter((df['idx_point'] == Asign_point) | (df['idx_point'] == elec_point))\n",
    "        #df_idx_dad_min_dis = df.filter(df['idx_point'] == 7) \n",
    "        #df5=df\n",
    "        #    .select(df['idx_point'], df['features'])\\\n",
    "        #    .withColumn('item_position',user_func(df.features,F.lit(num1)))\\\n",
    "        #    .orderBy(\"item_position\", ascending=True).limit(1).collect()\n",
    "        #idx_dad_min_dis = df_idx_dad_min_dis[0][0]\n",
    "        ##print(idx_dad_min_dis[0][0])       \n",
    "        #vec[idx_dad_min_dis:-1].append(num1) \n",
    "    return vec       \n",
    "\n",
    "def indexnext(idx_point1, ind_nxt_point1):\n",
    "    d=ind_nxt_point1\n",
    "    if idx_point1 == elec_point: \n",
    "        if (d<count_point):\n",
    "            d= d+1\n",
    "        else: d=0    \n",
    "    return d\n",
    "\n",
    "def update_father_child(x,tmp_next_Points,father_point):\n",
    "    if ((len(x)>2) and (tmp_next_Points in x[1:2])):\n",
    "        x[1]=father_point\n",
    "    return x\n",
    "\n",
    "\n",
    "def child_other_father(x1,list_num_childs1):\n",
    "    if (len(x1)>2):\n",
    "        if x1[0] not in list_num_childs1:  \n",
    "            return (x1[0]) \n",
    "    return -1\n",
    "\n",
    "\n",
    "#print('ls_clu_Pnt: ',ls_clu_Pnt.__str__()) # all element #print\n",
    "#print('ls_sel_pnt: ',ls_sel_pnt)\n",
    "\n",
    "#totoalindex=round(ir*(count_point * count_point),0)\n",
    "ir=0.2\n",
    "totoalindex=round(ir*(count_point * count_point),0)\n",
    "#print(\"totoalindex:\", totoalindex)\n",
    "\n",
    "index=0\n",
    "i=1\n",
    "while (len(ls_sel_pnt)< count_point) and (index<totoalindex):\n",
    "#while i<=100:\n",
    "#while i==1:\n",
    "    #rdd_sel_pnt=sc.parallelize(ls_sel_pnt)\n",
    "    #elec_point =  rdd_sel_pnt.takeSample(False, 1)\n",
    "    #elec_point= elec_point[0]\n",
    "    ##print(randbelow(10))\n",
    "    elec_point =  random.sample(ls_sel_pnt,1)[0]\n",
    "    rdd_cluster_pnt = sc.parallelize(ls_clu_Pnt)\n",
    "    #elec_point=0\n",
    "\n",
    "\n",
    "\n",
    "    index=index+1\n",
    "    print(\"-------------------------------------------index: \",index)\n",
    "    #print('elec_point: ',elec_point)\n",
    "\n",
    "\n",
    "    #filter_udf = udf(indexnext, IntegerType())\n",
    "    #df= df.withColumn(\"ind_nxt_point\", filter_udf(df['idx_point'], df['ind_nxt_point'])) #update index for point next\n",
    "    #df.show()\n",
    "    #df = df.filter(col(\"idx_point\").between(0, 0)).withColumn('ind_nxt_point',F.when(F.col('idx_point') == elec_point, 55).otherwise(88))\n",
    "    #tmp_index_nearset = df.filter(df['idx_point'] == elec_point).collect()[0][1] #+1:first point is self\n",
    "    #df.show()\n",
    "\n",
    "    list_index_nearset[elec_point]=list_index_nearset[elec_point]+1\n",
    "    ##print(\"list_index_nearset\",list_index_nearset)\n",
    "    tmp_index_nearset=list_index_nearset[elec_point]\n",
    "\n",
    "\n",
    "    ##print(\"tmp_index_nearset: \",tmp_index_nearset)\n",
    "\n",
    "\n",
    "\n",
    "    distance,index_dis = kdt_b.value.query(coords[elec_point], k=tmp_index_nearset+1)#+1:first point is self\n",
    "    #distance,index_dis = kdt_b.value.query(coords[elec_point], k=7)\n",
    "    ##print(distance,index_dis)  \n",
    "    ##print(\"vec_dis:\",index_dis)  \n",
    "    #tmp_next_Points=coords[index_dis[tmp_index_nearset]]\n",
    "    tmp_next_Points=index_dis[tmp_index_nearset]\n",
    "    ##print(index_dis[tmp_index_nearset])        \n",
    "    #print('tmp_next_Points: ',tmp_next_Points)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #rdd= sc.parallelize(ls_clu_Pnt)\n",
    "    #ls_clu_Pnt = rdd.map(lambda x: clu_update(x,tmp_next_Points)).collect()\n",
    "    if  tmp_next_Points in ls_sel_pnt: #N befor selected\n",
    "        #----\n",
    "        #if (elec_point in ls_sel_pnt) and (len(ls_sel_pnt)==3): continue #run 1 and two point are in initial_point\n",
    "        #Asign_point= deep_index(ls_clu_Pnt, elec_point, 0,1)[0][0] # jsut col 0 returned for check\n",
    "        if  elec_point not in ls_clu_Pnt[tmp_next_Points][2:]: #check (4,0) for (0,4) ,check else col 2 is father\n",
    "\n",
    "\n",
    "            #rdd = sc.parallelize(ls_clu_Pnt)#find elect father (check for e & n are in samecluster(no assignment))\n",
    "            rdd = rdd_cluster_pnt.filter(lambda x: (len(x)>2) and (elec_point in x[2:])).collect()\n",
    "            if (len(rdd) >0): \n",
    "                if rdd[0][1] == -1: father_elect= rdd[0][0]\n",
    "                else:  father_elect= rdd[0][1]\n",
    "            else:  father_elect=elec_point\n",
    "            #rdd = sc.parallelize(ls_clu_Pnt)#find tmp_next_Points father (check for e & n are in samecluster(no assignment))\n",
    "            rdd = rdd_cluster_pnt.filter(lambda x: (len(x)>2) and (tmp_next_Points in x[2:])).collect()\n",
    "            if (len(rdd) >0): \n",
    "                if rdd[0][1] == -1: father_tmp_next_Points= rdd[0][0]\n",
    "                else:  father_tmp_next_Points= rdd[0][1]\n",
    "            else:  father_tmp_next_Points=tmp_next_Points\n",
    "\n",
    "\n",
    "            #rdd = sc.parallelize(ls_clu_Pnt)#check for e & n are in samecluster(no assignment) \n",
    "            #rdd = rdd.map(lambda x: x).filter(lambda x: (len(x)>2 and (elec_point in x[2:]) and (tmp_next_Points2 in x[2:])) ).collect()\n",
    "            #rdd = rdd.map(lambda x: x).filter(lambda x: (len(x)>2).filter(lambda x: elec_point in x[2:] and (tmp_next_Points2 in x[2:])) ).collect()\n",
    "            #if (len(rdd) ==0):\n",
    "            if (father_elect != father_tmp_next_Points):\n",
    "\n",
    "                Asign_point= deep_index(ls_clu_Pnt, tmp_next_Points, 2,len(ls_clu_Pnt)) #find num index in cluster(ls_clu_Pnt[2:] ,for two father)\n",
    "                #print('Asign_point(befor father): ',Asign_point) #=>[(7, 1)] (7 -> col 0(father) , 1 -> index tmp_next_Points in [])\n",
    "\n",
    "                if len(Asign_point)>0: #N befor selected and is not one of initial point\n",
    "                    #print(\"N befor selected and is not one of initial point\")\n",
    "                    father_befor=Asign_point[0][0]\n",
    "                    father_new= elec_point  \n",
    "                    #point_father = [coords[father_befor],coords[father_new]]\n",
    "                    tree_father = spatial.cKDTree([coords[father_befor],coords[father_new]],leafsize=16)\n",
    "                    distance,index_dis = tree_father.query(coords[tmp_next_Points])\n",
    "                    ##print(distance,index_dis)\n",
    "                    if (index_dis==0): \n",
    "                        idx_dad_min_dis=father_befor \n",
    "                        idx_dad_max_dis=father_new \n",
    "                    else: \n",
    "                        idx_dad_min_dis=father_new \n",
    "                        idx_dad_max_dis=father_befor \n",
    "                    #print(\"idx_dad_min_dis: \",idx_dad_min_dis)\n",
    "\n",
    "                    if (tmp_next_Points) not in ls_clu_Pnt[idx_dad_min_dis][2:]:\n",
    "                        #rdd = sc.parallelize(ls_clu_Pnt)\n",
    "                        rdd = rdd_cluster_pnt.filter(lambda x: (len(x)>2 and (elec_point in x[2:])) ).collect() #find index father (in col 2 of list)\n",
    "                        if (len(rdd) >0):\n",
    "                            if rdd[0][1] == -1: \n",
    "                                father_point= rdd[0][0]\n",
    "                            else:\n",
    "                                father_point= rdd[0][1]\n",
    "                            ls_clu_Pnt[elec_point].append(father_point)  #(col1:point number , col2: father point, col3: N ...)     \n",
    "                        else:\n",
    "                            if (len(ls_clu_Pnt[idx_dad_min_dis])==1):                \n",
    "                                ls_clu_Pnt[idx_dad_min_dis].insert(1,-1)\n",
    "                        #print(\"end 5\")\n",
    "                        ls_clu_Pnt[idx_dad_min_dis].append(tmp_next_Points) \n",
    "                        if (tmp_next_Points in ls_clu_Pnt[idx_dad_max_dis][2:]):\n",
    "                            ls_clu_Pnt[idx_dad_max_dis].remove(tmp_next_Points) \n",
    "                            if ((len(ls_clu_Pnt[idx_dad_max_dis])==2) and (ls_clu_Pnt[idx_dad_max_dis][1] !=-1)):\n",
    "                                ls_clu_Pnt[idx_dad_max_dis].remove(ls_clu_Pnt[idx_dad_max_dis][1]) #remove child => len=2 => remve childa's father\n",
    "                        #**************---update childs(find childs & assign other father(elected)    \n",
    "                        #--find elected's father \n",
    "                        #rdd = sc.parallelize(ls_clu_Pnt)\n",
    "                        rdd = rdd_cluster_pnt.filter(lambda x: (len(x)>2 and (elec_point in x[2:])) ).collect() #find index father (in col 2 of list)\n",
    "                        if (len(rdd) >0):\n",
    "                            if rdd[0][1] == -1: \n",
    "                                father_point= rdd[0][0]\n",
    "                            else:\n",
    "                                father_point= rdd[0][1]\n",
    "                            #if (len(ls_clu_Pnt[elec_point])==1):\n",
    "                                #ls_clu_Pnt[elec_point].insert(1,father_point) #(col1:point number , col2: father point, col3: N ...) \n",
    "                            #ls_clu_Pnt[tmp_next_Points][1]=father_point\n",
    "                        else:\n",
    "                            #if (elec_point not in initial_Point):\n",
    "                            #ls_clu_Pnt[elec_point].insert(1,-1)\n",
    "                            father_point=-1\n",
    "                        #--\n",
    "                        if (len(ls_clu_Pnt[tmp_next_Points])>2):\n",
    "                            #--find list childs of childs tmp_next_Points & update fathers\n",
    "                            #print(\"father point:\",father_point)\n",
    "                            ls_clu_Pnt = rdd_cluster_pnt.map(lambda x: update_father_child(x,tmp_next_Points,father_point)).collect()#[1:2] searech all items with index[1] only\n",
    "\n",
    "                            #rdd = sc.parallelize(ls_clu_Pnt)\n",
    "                            #list_childs = rdd_cluster_pnt.filter(lambda x: ((len(x)>2) and (tmp_next_Points in x[1:2])) ).collect()#[1:2] searech all items with index[1] only\n",
    "                            ##print('list_childs of childs ',tmp_next_Points,': ', list_childs)\n",
    "                            #if (len(list_childs) >0):\n",
    "                            #    for x in list_childs: x[1]= father_point\n",
    "                            #--\n",
    "                            #--remove (befor electe) and reasign new elected\n",
    "                            if (ls_clu_Pnt[tmp_next_Points][1]==-1):\n",
    "                                initial_Point.remove(tmp_next_Points)\n",
    "                                elec_point_again =  sc.parallelize(list(set(list(range(len(ls_clu_Pnt))))-set(ls_sel_pnt))).takeSample(False, 1) # reasignment\n",
    "                                elec_point_again= elec_point_again[0]\n",
    "                                #print('elec_point_again: ',elec_point_again)\n",
    "                                ls_clu_Pnt[elec_point_again].insert(1,-1)\n",
    "                                ls_sel_pnt.append(elec_point_again)\n",
    "                                initial_Point.append(elec_point_again)  \n",
    "                                #print('initial_Point new:', initial_Point)                            \n",
    "                            #--\n",
    "                        #if (len(ls_clu_Pnt[elec_point])==1):\n",
    "                        #    ls_clu_Pnt[elec_point].insert(1,father_point) #(col1:point number , col2: father point, col3: N ...) \n",
    "                        #ls_clu_Pnt[elec_point].append(tmp_next_Points)  \n",
    "                        #if (len(ls_clu_Pnt[tmp_next_Points])>2):\n",
    "                        #    ls_clu_Pnt[tmp_next_Points][1]=father_point\n",
    "                        #***************------   \n",
    "\n",
    "                        #----update childs(find childs & assign to other elected)    \n",
    "                        ##def child_other_father(x1,list_num_childs1):\n",
    "                        ##    if (len(x1)>2):\n",
    "                        ##        if x1[0] not in list_num_childs1:  \n",
    "                        ##            return (x1[0]) \n",
    "                        ##    return -1\n",
    "                        #if len(ls_clu_Pnt[tmp_next_Points])>2:\n",
    "                        #    idx_cluster_child= ls_clu_Pnt[tmp_next_Points][1]\n",
    "                        #    #print('idx_cluster_child: ',idx_cluster_child)\n",
    "                        #    rdd2 = sc.parallelize(ls_clu_Pnt)#select all points of cluster father(col2)-> find child_other_father and remove them\n",
    "                        #    list_samecluster_full_childs = rdd2.map(lambda x: x).filter(lambda x: ((len(x)>2) and (idx_cluster_child in x[1:2]) )).collect() #find index father (in col 2 of list)\n",
    "                        #    #print('list_samecluster_full_childs:' , list_samecluster_full_childs)\n",
    "                        #    list_num_childs=[]\n",
    "                        #    for x in list_samecluster_full_childs: list_num_childs.append(x[2:]) #all child's\n",
    "                        #    l= sum(list_num_childs, []) #convert [[4,5],[3]] to [4,5,3]\n",
    "                        #    #print('list_num_childs: ',list_num_childs)\n",
    "                        #    rdd = sc.parallelize(list_samecluster_full_childs)\n",
    "                        #    list_child_other_father = rdd.map(lambda x: child_other_father(x, list_num_childs)).collect()\n",
    "                        #    #print('list_child_other_father: ',list_child_other_father)\n",
    "                        #    while -1 in list_child_other_father:  list_child_other_father.remove(-1)\n",
    "                        #    list_child_other_father.remove(tmp_next_Points) # father all child's\n",
    "                        #    #print('list_child_other_father: ',list_child_other_father)\n",
    "                        #    list_childs = list(filter(lambda list_samecluster_full_childs: list_samecluster_full_childs[0] not in list_child_other_father, list_samecluster_full_childs))\n",
    "                        #    #print('list_childs ',idx_cluster_child,': ', list_childs)\n",
    "                        #    for x in list_childs: x[1]=elec_point\n",
    "                        #    #print('list_childs ',idx_cluster_child,' (update): ', list_childs)\n",
    "                        #-----------------\n",
    "\n",
    "\n",
    "\n",
    "                else: # N is one of initial point\n",
    "                    if (len(ls_clu_Pnt[tmp_next_Points])<3): #N subset initial_Point & befor not selected(dont have child)\n",
    "                        #print('initial_Point:', initial_Point)\n",
    "                        #if (len(ls_clu_Pnt[elec_point])==1):                \n",
    "                        #    ls_clu_Pnt[elec_point].insert(1,-1)\n",
    "                        #--find elected's father \n",
    "                        #rdd = sc.parallelize(ls_clu_Pnt)\n",
    "                        rdd = rdd_cluster_pnt.filter(lambda x: (len(x)>2 and (elec_point in x[2:])) ).collect() #find index father (in col 2 of list)\n",
    "                        if (len(rdd) >0):\n",
    "                            if rdd[0][1] == -1: \n",
    "                                father_point= rdd[0][0]\n",
    "                            else:\n",
    "                                father_point= rdd[0][1]\n",
    "                            if (len(ls_clu_Pnt[elec_point])==1):\n",
    "                                #ls_clu_Pnt[elec_point].append(father_point)  #(col1:point number , col2: father point, col3: N ...) \n",
    "                                ls_clu_Pnt[elec_point].insert(1,father_point)\n",
    "                        #--\n",
    "                        ls_clu_Pnt[elec_point].append(tmp_next_Points)  \n",
    "                        ls_clu_Pnt[tmp_next_Points].remove(-1) \n",
    "                        initial_Point.remove(tmp_next_Points)\n",
    "                        elec_point_again =  sc.parallelize(list(set(list(range(len(ls_clu_Pnt))))-set(ls_sel_pnt))).takeSample(False, 1) # reasignment\n",
    "                        elec_point_again= elec_point_again[0]\n",
    "                        #print('elec_point_again: ',elec_point_again)\n",
    "                        ls_clu_Pnt[elec_point_again].insert(1,-1)\n",
    "                        ls_sel_pnt.append(elec_point_again)\n",
    "                        initial_Point.append(elec_point_again)  \n",
    "                        #print('initial_Point new:', initial_Point)\n",
    "\n",
    "                    else: #((len(ls_clu_Pnt[tmp_next_Points])<3): #N subset initial_Point & befor..)\n",
    "                    #N(tmp_next_Points) subset initial_Point & befor selected(have child)\n",
    "                        #print('N(tmp_next_Points) subset initial_Point & befor selected(have child)')\n",
    "                        #---------------update childs(find childs & assign other father(elected)    \n",
    "                        #--find elected father \n",
    "                        #rdd = sc.parallelize(ls_clu_Pnt)\n",
    "                        rdd = rdd_cluster_pnt.filter(lambda x: (len(x)>2 and (elec_point in x[2:])) ).collect() #find index father (in col 2 of list)\n",
    "                        father_point=-100 # for know of wrong\n",
    "                        if (len(rdd) >0): # e sure have father\n",
    "                            if rdd[0][1] == -1: \n",
    "                                father_point= rdd[0][0]\n",
    "                            else:\n",
    "                                father_point= rdd[0][1]\n",
    "                            #if (len(ls_clu_Pnt[elec_point])==1):\n",
    "                                #ls_clu_Pnt[elec_point].insert(1,father_point) #(col1:point number , col2: father point, col3: N ...) \n",
    "                            #ls_clu_Pnt[tmp_next_Points][1]=father_point\n",
    "                        else: # e is elected\n",
    "                            #rdd = rdd.map(lambda x: x).filter(lambda x: (len(x)>2 and (elec_point in x[0:1])) ).collect() #find index father (in col 2 of list)\n",
    "                            #if (len(ls_clu_Pnt[elec_point])) ==2):# e is elected\n",
    "                            #    father_point=ls_clu_Pnt[tmp_next_Points][1]\n",
    "                            #else:\n",
    "                                #father_point=ls_clu_Pnt[tmp_next_Points][1]\n",
    "                            father_point=elec_point\n",
    "\n",
    "                        #--\n",
    "                        if (len(ls_clu_Pnt[tmp_next_Points])>2):\n",
    "                            #--find list childs of childs tmp_next_Points & update fathers\n",
    "                            #print(\"father point:\",father_point)\n",
    "                            ls_clu_Pnt = rdd_cluster_pnt.map(lambda x: update_father_child(x,tmp_next_Points,father_point)).collect()#[1:2] searech all items with index[1] only\n",
    "\n",
    "                            #rdd = sc.parallelize(ls_clu_Pnt)\n",
    "                            #list_childs = rdd_cluster_pnt.filter(lambda x: ((len(x)>2) and (tmp_next_Points in x[1:2])) ).collect()#[1:2] searech all items with index[1] only\n",
    "                            ##print('list_childs of childs ',tmp_next_Points,': ', list_childs)\n",
    "                            ##print(\"father point:\",father_point)\n",
    "                            #if (len(list_childs) >0):\n",
    "                            #    for x in list_childs: \n",
    "                            #        #print(\"child:\",x[1])\n",
    "                            #        x[1]= father_point\n",
    "                            #        #print(\"update child father:\",x[1])\n",
    "                            #--\n",
    "                            #--remove (befor electe) and reasign new elected\n",
    "                            if (ls_clu_Pnt[tmp_next_Points][1]==-1):\n",
    "                                initial_Point.remove(tmp_next_Points)\n",
    "                                elec_point_again =  sc.parallelize(list(set(list(range(len(ls_clu_Pnt))))-set(ls_sel_pnt))).takeSample(False, 1) # reasignment\n",
    "                                elec_point_again= elec_point_again[0]\n",
    "                                #print('elec_point_again: ',elec_point_again)\n",
    "                                ls_clu_Pnt[elec_point_again].insert(1,-1)\n",
    "                                ls_sel_pnt.append(elec_point_again)\n",
    "                                initial_Point.append(elec_point_again)  \n",
    "                                #print('initial_Point new:', initial_Point)                            \n",
    "                            #--\n",
    "                        if (len(ls_clu_Pnt[elec_point])==1):\n",
    "                            ls_clu_Pnt[elec_point].insert(1,father_point) #(col1:point number , col2: father point, col3: N ...) \n",
    "                        ls_clu_Pnt[elec_point].append(tmp_next_Points)  \n",
    "                        if (len(ls_clu_Pnt[tmp_next_Points])>2):\n",
    "                            ls_clu_Pnt[tmp_next_Points][1]=father_point\n",
    "                        #-----------------   \n",
    "            #else:   #(len(rdd) ==0)                 \n",
    "                #print(\"elec_point & tmp_next_Points are in same cluster\")\n",
    "\n",
    "        else: # (elec_point not in ls_clu_Pnt[tmp_next_Points][2:]: #check (4,0) for (0,4))\n",
    "            #if (elec_point in ls_sel_pnt) and (len(ls_sel_pnt)==3):  #run 1 and two point are in initial_point\n",
    "            if (len(ls_sel_pnt)==3):  #run 1 and two point are in initial_point\n",
    "                #print('elec_point & tmp_next_Points are in initial_Point for first run')\n",
    "                ls_clu_Pnt[elec_point].append(tmp_next_Points)  \n",
    "                ls_clu_Pnt[tmp_next_Points].remove(-1) \n",
    "                initial_Point.remove(tmp_next_Points)\n",
    "                elec_point_again =  sc.parallelize(list(set(list(range(len(ls_clu_Pnt))))-set(ls_sel_pnt))).takeSample(False, 1) # reasignment\n",
    "                elec_point_again= elec_point_again[0]\n",
    "                #print('elec_point_again: ',elec_point_again)\n",
    "                ls_clu_Pnt[elec_point_again].insert(1,-1)\n",
    "                ls_sel_pnt.append(elec_point_again)\n",
    "                initial_Point.append(elec_point_again)  \n",
    "                #print('initial_Point new:', initial_Point)\n",
    "            #else: #((elec_point in ls_sel_pnt) and (len(ls_sel_pnt)==3):)\n",
    "\n",
    "    else: #(tmp_next_Points in ls_sel_pnt: #N befor selected)\n",
    "        #Asign_point= deep_index2(ls_clu_Pnt, tmp_next_Points) #find num index in cluster(ls_clu_Pnt[1:] ,for two father)\n",
    "        ##print(Asign_point) #=>[(7, 1)] (1->index 3)\n",
    "        #Asign_point=Asign_point[0][0]\n",
    "        ##print(Asign_point) #=> 7 (7 -> cluster dad(Asign))            \n",
    "        #print(\"tmp_next_Points without father & child\")\n",
    "        #rdd = sc.parallelize(ls_clu_Pnt)\n",
    "        #rdd = rdd.map(lambda x: x).filter(lambda x: (len(x)>1 and (x[0]==tmp_next_Points)  and (x[1]!=-1))).collect() #find index father (in col 2 of list)\n",
    "        #rdd = rdd.map(lambda x: x).filter(lambda x: (len(x)>2 and (elec_point in x[2:])) ).collect() #find index father (in col 2 of list)\n",
    "        rdd = rdd_cluster_pnt.filter(lambda x: (len(x)>0 and (elec_point in x[2:])) ).collect() #find index father (in col 2 of list)\n",
    "        if (len(rdd) >0):\n",
    "            if rdd[0][1] == -1: \n",
    "                father_point= rdd[0][0]\n",
    "            else:\n",
    "                father_point= rdd[0][1]\n",
    "            if (len(ls_clu_Pnt[elec_point])==1):\n",
    "                ls_clu_Pnt[elec_point].append(father_point)  #(col1:point number , col2: father point, col3: N ...) \n",
    "        else:\n",
    "            if (elec_point not in initial_Point):\n",
    "                ls_clu_Pnt[elec_point].insert(1,-1)\n",
    "\n",
    "        ls_clu_Pnt[elec_point].append(tmp_next_Points)       \n",
    "        ls_sel_pnt.append(tmp_next_Points)\n",
    "\n",
    "\n",
    "\n",
    "    #print('ls_clu_Pnt: ',ls_clu_Pnt.__str__())\n",
    "    #print('ls_sel_pnt: ',ls_sel_pnt.__str__())\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "#print(\"-----------#print clusters-------------\")\n",
    "rdd = sc.parallelize(ls_clu_Pnt)#find elect father (check for e & n are in samecluster(no assignment))\n",
    "list_all_cluster = rdd.filter(lambda x: (len(x)>1) and (x[1]==-1)).collect()\n",
    "##print(list_all_cluster7)\n",
    "list_all_full_cluster=[]\n",
    "if (len(list_all_cluster) >0): \n",
    "    for y in list_all_cluster: \n",
    "        ##print(y)\n",
    "        list_cluster=[]\n",
    "        list_cluster = rdd.filter(lambda x: (len(x)>1) and (y[0]==x[1])).collect()\n",
    "        list_cluster.insert(0,y)\n",
    "        ##print(list_cluster)\n",
    "        for x in list_cluster: x.remove(x[1])\n",
    "        list_cluster= sum(list_cluster, []) #convert [[4,5],[3]] to [4,5,3]\n",
    "        ##print(list_cluster)\n",
    "        list_cluster = list( dict.fromkeys(list_cluster) )\n",
    "        ##print(list_cluster)\n",
    "        list_all_full_cluster.append(list_cluster)\n",
    "\n",
    "print(\"list_all_full_cluster:\",list_all_full_cluster)\n",
    "#coords2 = [(2, 4),(4, 5), (3,8), (6,7), (1,5), (6,2), (3,7), (8,9), (5,8), (3,6)]\n",
    "##print(\"cords2: \",coords2)\n",
    "\n",
    "list_x=[]\n",
    "list_y=[]\n",
    "list_c=[]\n",
    "for xx in list_all_full_cluster:\n",
    "    for yy in xx:\n",
    "        ##print(yy)\n",
    "        list_c.append(list_all_full_cluster.index(xx))\n",
    "        list_x.append(coords[yy][0])\n",
    "        list_y.append(coords[yy][1])\n",
    "\n",
    "##print(\"list_c:\",list_c)\n",
    "##print(\"list_x:\",list_x)\n",
    "##print(\"list_y:\",list_y)\n",
    "list_xy=[]\n",
    "list_xy.append(list_x)\n",
    "list_xy.append(list_y)\n",
    "##print(list_xy)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "categories = np.array(list_c)\n",
    "colormap = np.array(['r', 'g', 'b'])\n",
    "plt.scatter(list_xy[0], list_xy[1],  c=colormap[categories])\n",
    "plt.savefig('ScatterClassPlot.png')\n",
    "plt.show()    \n",
    "\n",
    "\n",
    "#sc.stop()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkContext.stop of <SparkContext master=local appName=SQL_DataFrame>>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
