{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from NPIR import NPIR\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "import warnings\n",
    "from collections import Counter as Cs\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "########\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "########\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from operator import *\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.types import IntegerType, FloatType, BooleanType, StringType, StructType,\\\n",
    "StructField,ArrayType, DataType\n",
    "from pyspark.sql.functions import udf, log, rand, monotonically_increasing_id, col, broadcast,\\\n",
    "greatest, desc, asc, row_number, avg, mean, least, struct, lit, sequence, sum\n",
    "from functools import reduce\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, SQLContext, Window, Row, DataFrame\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.sql.broadcastTimeout\", \"30000s\").\\\n",
    "config(\"spark.network.timeout\",\"30000s\").config(\"spark.executor.heartbeatInterval\",\"12000000ms\").\\\n",
    "config(\"spark.storage.blockManagerSlaveTimeoutMs\",\"12000001ms\").config(\"spark.driver.maxResultSize\",\"12g\").\\\n",
    "config(\"spark.default.parallelism\", \"100\").config(\"spark.memory.offHeap.enabled\",\"true\").\\\n",
    "config(\"spark.memory.offHeap.size\", \"25g\").appName(\"NPIR_Parallel\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.785319</td>\n",
       "      <td>5.568516</td>\n",
       "      <td>-10.649756</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.565005</td>\n",
       "      <td>4.564475</td>\n",
       "      <td>-9.979172</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.497358</td>\n",
       "      <td>-5.706487</td>\n",
       "      <td>-8.941820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.507116</td>\n",
       "      <td>4.077689</td>\n",
       "      <td>-11.093956</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.918326</td>\n",
       "      <td>-1.266070</td>\n",
       "      <td>-1.457767</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1          2  label\n",
       "0 -1.785319  5.568516 -10.649756      0\n",
       "1 -2.565005  4.564475  -9.979172      0\n",
       "2 -4.497358 -5.706487  -8.941820      1\n",
       "3 -1.507116  4.077689 -11.093956      0\n",
       "4 -5.918326 -1.266070  -1.457767      2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1 = make_blobs(n_samples=1000, centers=3, n_features=3,\n",
    "                random_state=1)\n",
    "data = pd.DataFrame(x1)\n",
    "data['label'] = y1\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-6.178202</td>\n",
       "      <td>-2.290967</td>\n",
       "      <td>-0.949075</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>-5.823324</td>\n",
       "      <td>-2.282588</td>\n",
       "      <td>-1.217316</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>-7.821134</td>\n",
       "      <td>-4.202928</td>\n",
       "      <td>-1.475274</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>-4.981322</td>\n",
       "      <td>-6.513678</td>\n",
       "      <td>-8.113955</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>-4.924054</td>\n",
       "      <td>-7.057942</td>\n",
       "      <td>-7.374577</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2  label\n",
       "9   -6.178202 -2.290967 -0.949075      2\n",
       "578 -5.823324 -2.282588 -1.217316      2\n",
       "170 -7.821134 -4.202928 -1.475274      2\n",
       "594 -4.981322 -6.513678 -8.113955      1\n",
       "886 -4.924054 -7.057942 -7.374577      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = shuffle(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('blobs3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----+\n",
      "|                  0|                  1|label|\n",
      "+-------------------+-------------------+-----+\n",
      "| -6.387419071034336| -9.187990381898826|    2|\n",
      "| -8.005178595989918|-7.2200516657858405|    2|\n",
      "| -9.697294242434808|-3.5225390707496924|    1|\n",
      "| -1.279403340386375| 2.5984596752053735|    0|\n",
      "| -8.563066013141189|-2.9369284461861698|    1|\n",
      "| -9.874034130661965| -6.478907984278638|    1|\n",
      "| 0.6679093864997019| 3.9098652430890275|    0|\n",
      "| -8.825110289032803| -3.781733114789269|    1|\n",
      "| -8.995818953272746| -8.344036741165064|    2|\n",
      "| -8.547061454068198|  -4.30459999107529|    1|\n",
      "| -6.033181639480226| -8.561204983658731|    2|\n",
      "| -7.465835003635627| -9.686186295830733|    2|\n",
      "| -5.713231338882324| -7.549527445975742|    2|\n",
      "|-10.870568282036668| -4.963228932719754|    1|\n",
      "| -6.724671113424062| -7.772678812956407|    2|\n",
      "| -10.33555446704167| -4.229877104261299|    1|\n",
      "| -8.973272284198908|-4.1611574672378655|    1|\n",
      "| -6.926612635252511| -8.131169868850513|    2|\n",
      "| -1.871923870212913|  5.022219985167822|    0|\n",
      "| -0.956842306377999|  4.583880419070251|    0|\n",
      "+-------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read csv\n",
    "data_spark_df = spark.read.format('csv').option('header','True').option('index','False').load('blobs3.csv')\n",
    "# data_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, 0: string, 1: string, label: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_spark_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 0: string (nullable = true)\n",
      " |-- 1: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spark_df = data_spark_df.select('0', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name = ['first', 'second']\n",
    "data_spark_rdd = data_spark_df.toDF(*new_name).rdd.filter(lambda x:x)\n",
    "data_spark_df = data_spark_df.toDF(*new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "spark.conf.set('spark.jars.packages','com.databricks:spark-cav_2.11')\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"False\")\n",
    "sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1,y1 = make_blobs(n_samples=100, centers=3, n_features=2,\n",
    "#                 random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = data_spark_df.count()\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100000/400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IR: The indexing ratio to be used for generating the maximum index\n",
    "IR = 0.2\n",
    "#The number of iteration i\n",
    "i = 10\n",
    "k = 3 #k: Number of clusters\n",
    "# count = Cs()\n",
    "chunk = 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = timer()\n",
    "# kmeans = KMeans(n_clusters=k, random_state=0).fit(x)\n",
    "# end = timer()\n",
    "# print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data_spark = data_spark_df.withColumn('index_column_name', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "# data_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first', 'second', 'index_column_name']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chunkLabel', 'old label', 'count', '1', '2')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderheadr = ['chunkLabel', 'old label', 'count']\n",
    "leaderheadr.extend([str(x) for x in range(1, len(data_spark.columns))])\n",
    "leaderheadr = tuple(leaderheadr)\n",
    "leaderheadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chunkLabel', 'label')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelsheader = ('chunkLabel', 'label')\n",
    "labelsheader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time HH:MM:SS: 0:26:28.420300\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "\n",
    "labels = sqlContext.createDataFrame([np.full(len(labelsheader), np.nan).tolist()],labelsheader)\n",
    "labels = labels.na.drop()\n",
    "\n",
    "leaders = sqlContext.createDataFrame([np.full(len(leaderheadr), np.nan).tolist()],leaderheadr)\n",
    "leaders = leaders.na.drop()\n",
    "\n",
    "ii = 0\n",
    "for z in range(0, points, chunk):\n",
    "    j = z + chunk\n",
    "    data = data_spark.where(col(\"index_column_name\").between(z, j-1)).toPandas()\n",
    "    data.drop(\"index_column_name\",axis=1,inplace=True)\n",
    "    data = data.astype(float)\n",
    "    from NPIR import NPIR\n",
    "    label = NPIR(data.values,k,IR,i)\n",
    "    data['labels'] = label\n",
    "    \n",
    "    # Adding to pyspard label\n",
    "    chunklabel = np.full(len(label), ii).tolist()\n",
    "    labelDF = [(x, y) for x, y in zip(chunklabel, label)]\n",
    "    labelsDF = sqlContext.createDataFrame(labelDF,labelsheader)\n",
    "    labels = unionAll(labels, labelsDF)\n",
    "\n",
    "    leader = []\n",
    "    f = list(Cs(label))\n",
    "    f.sort()\n",
    "    for i in f:\n",
    "        leader.append([round(np.mean(z), 4) for z in data[data['labels']==i].values[:,:-1].T])\n",
    "    del data\n",
    "    del NPIR\n",
    "    \n",
    "    # Adding to pyspark leaders\n",
    "    for x in range(len(leader)):\n",
    "        x1 = [ii, x, Cs(label)[x]]\n",
    "        x1.extend(leader[x])\n",
    "        leader[x] = x1\n",
    "    leaderDF = sqlContext.createDataFrame(leader,leaderheadr)\n",
    "    leaders = unionAll(leaders, leaderDF)\n",
    "    ii += 1\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cs(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7666b7c9909d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Build the model (cluster the data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsedData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializationMode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"random\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Execution time HH:MM:SS:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/mllib/clustering.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, rdd, k, maxIterations, initializationMode, seed, initializationSteps, epsilon, initialModel)\u001b[0m\n\u001b[1;32m    349\u001b[0m         model = callMLlibFunc(\"trainKMeansModel\", rdd.map(_convert_to_vector), k, maxIterations,\n\u001b[1;32m    350\u001b[0m                               \u001b[0minitializationMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializationSteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                               clusterInitialModel)\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclusterCenters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mKMeansModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Load and parse the data\n",
    "# data = sc.textFile(\"data/mllib/kmeans_data.txt\")\n",
    "# parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "parsedData = leaders.select(['1', '2']).rdd.map(list)\n",
    "\n",
    "start = timer()\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, k, maxIterations=20, initializationMode=\"random\")\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "# def error(point):\n",
    "#     center = clusters.centers[clusters.predict(point)]\n",
    "#     return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "# WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "# print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters.centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save and load model\n",
    "# clusters.save(sc, \"KMeansModel\")\n",
    "# sameModel = KMeansModel.load(sc, \"KMeansModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import array\n",
    "# from math import sqrt\n",
    "\n",
    "# from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "# # Load and parse the data\n",
    "# data = sc.textFile(\"data/mllib/kmeans_data.txt\")\n",
    "# parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "\n",
    "# # Build the model (cluster the data)\n",
    "# clusters = KMeans.train(parsedData, 2, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "# # Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "# def error(point):\n",
    "#     center = clusters.centers[clusters.predict(point)]\n",
    "#     return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "# WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "# print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "# # Save and load model\n",
    "# clusters.save(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")\n",
    "# sameModel = KMeansModel.load(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c1x c2y c3z\n",
    "# /\n",
    "# (c1 + c2 + c3)\n",
    "\n",
    "# sum = (c1 + c2 + c3)\n",
    "# leader[c1/sum*x +c2/sum*y+ c3/sum*z]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaders = leaders.select(['chunkLabel','old label','1','2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_spark = leaders.withColumn(\"first_numeric\", leaders[\"1\"].cast(FloatType()))\n",
    "# data_spark = data_spark.withColumn(\"second_numeric\", data_spark[\"2\"].cast(FloatType()))\n",
    "# data_spark = data_spark.drop('1').drop('2').drop('chunkLabel').drop('old label')\n",
    "# # #### Compute weighted average\n",
    "# # count = data_spark.select('count')\n",
    "# # ####\n",
    "\n",
    "# data_spark = data_spark.drop('1').drop('2').drop('chunkLabel').drop('old label').drop('count')\n",
    "# data_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = count.withColumn('Cindex', row_number().\\\n",
    "#                                           over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "# count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def MyCheckUpdate(a, b, c, d):\n",
    "#     a = float(a)\n",
    "#     b = float(b)\n",
    "#     c = float(c)\n",
    "#     d = float(d)\n",
    "#     res = (a-c) + (b-d)\n",
    "#     if res == 0:\n",
    "#         return 1.0\n",
    "#     return 0.0\n",
    "\n",
    "# check_centroid = udf(lambda x,y,z,r: MyCheckUpdate(x,y,z,r), FloatType())\n",
    "\n",
    "def squaree1(c,u,f,g):\n",
    "    c = float(c)\n",
    "    u = float(u)\n",
    "    f = float(f)\n",
    "    g = float(g)\n",
    "    array1 = np.array([c,u])\n",
    "    array2 = np.array([f,g])\n",
    "    dist = np.linalg.norm(array1-array2)\n",
    "    dist = dist.item()\n",
    "    return dist\n",
    "\n",
    "squaree_spark1 = udf(lambda x,y,z,r: squaree1(x,y,z,r), FloatType())\n",
    "sqlContext.sql(\"SET spark.sql.autoBroadcastJoinThreshold = -1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# c = -8\n",
    "# u = -9\n",
    "# f = -7\n",
    "# g = -8\n",
    "# array1 = np.array([c,u])\n",
    "# array2 = np.array([f,g])\n",
    "# dist = np.linalg.norm(array1-array2)\n",
    "# dist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_centroid = data_spark.sample(False, 0.4,seed = 0).limit(1).cache()\n",
    "\n",
    "# new_name = ['x','y']\n",
    "# df_centroid = df_centroid.toDF(*new_name)\n",
    "\n",
    "# #just for first round\n",
    "# #first time\n",
    "# i = 0\n",
    "# data_cent = data_spark.join(broadcast(df_centroid))\n",
    "# data_cent1 = data_cent.withColumn(str(i),squaree_spark1(data_cent.columns[0],data_cent.columns[1],\n",
    "#                                               data_cent.columns[2*i+2],data_cent.columns[2*i+3]))\n",
    "# data_cent2 = data_cent1.drop(data_cent1.columns[i+2]).drop(data_cent1.columns[i+3])\n",
    "# data_cent3 = data_cent2.withColumn('mindist',col(str(i)))\n",
    "# data_cent4 = data_cent3.withColumn('mindist1',least(data_cent3.columns[i+2], col('mindist')))\n",
    "# data_cent4 = data_cent4.drop('mindist')\n",
    "# data_cent5 = data_cent4.withColumnRenamed('mindist1','mindist')\n",
    "# next_selected = data_cent5.orderBy(desc('mindist')).limit(1).select(data_cent5.columns[0:2])#1:3\n",
    "\n",
    "# df_centroid = df_centroid.union(next_selected)\n",
    "# u = [str(i)+'x',str(i)+'y']\n",
    "# next_selected = next_selected.toDF(*u)\n",
    "# def initial_centroids(next_selected,data_cent_5_persist, i):\n",
    "#     data_cent6 = data_cent_5_persist.join(broadcast(next_selected))\n",
    "#     data_cent6 = data_cent6.withColumn(str(i),squaree_spark1(data_cent6.columns[0],data_cent6.columns[1],\n",
    "#                                              data_cent6.columns[i+3],data_cent6.columns[i+4]))#+4 +5\n",
    "#     data_cent6 = data_cent6.drop(data_cent6.columns[i+3]).drop(data_cent6.columns[i+4])#+4 +5\n",
    "#     data_cent6 = data_cent6.withColumn('mindist1',least(data_cent6.columns[i+3], col('mindist')))#4\n",
    "#     data_cent6 = data_cent6.drop('mindist')\n",
    "#     data_cent6 = data_cent6.withColumnRenamed('mindist1','mindist')\n",
    "#     next_cent = data_cent6.orderBy(desc('mindist')).limit(1).select(data_cent6.columns[0:2])#1:3\n",
    "#     return next_cent,data_cent6\n",
    "\n",
    "\n",
    "# data_cent_5_persist = data_cent5.persist(StorageLevel.MEMORY_ONLY_2)\n",
    "\n",
    "\n",
    "# # start = timer()\n",
    "# for i in range(1,k-1):\n",
    "#     next_selected, data_cent_5_persist = initial_centroids(next_selected,data_cent_5_persist, i)\n",
    "#     u = [str(i)+'x',str(i)+'y']\n",
    "#     next_selected = next_selected.toDF(*u)\n",
    "\n",
    "# # end = timer()\n",
    "# # print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))\n",
    "\n",
    "\n",
    "\n",
    "# i= k-1\n",
    "# data_cent11 = data_cent_5_persist.join(broadcast(next_selected))\n",
    "# data_cent11 = data_cent11.withColumn(str(i),squaree_spark1(data_cent11.columns[0],data_cent11.columns[1],\\\n",
    "#                                                            data_cent11.columns[k+2],data_cent11.columns[k+3]))\n",
    "# data_cent11 = data_cent11.drop('mindist').drop(data_cent11.columns[k+2]).drop(data_cent11.columns[k+3])\n",
    "\n",
    "\n",
    "# def FindMinCOl( *row_list):\n",
    "#     ind = row_list.index(min(*row_list))\n",
    "#     return int(ind)\n",
    "\n",
    "# find_min_val_name = udf(FindMinCOl, IntegerType())\n",
    "# data_cent14 = data_cent11.withColumn('defined_cluster', find_min_val_name(*data_cent11.columns[2:3+k]))\n",
    "# data_cent14 = data_cent14.select('first_numeric','second_numeric','defined_cluster')\n",
    "\n",
    "\n",
    "# # #### Compute weighted average\n",
    "# # data_cent14 = data_cent14.withColumn('index', row_number().\\\n",
    "# #                                           over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "# # data_cent14 = (data_cent14.join(count, (col('Cindex') == col('index')),\\\n",
    "# #           \"inner\")).drop('index').drop('Cindex')\n",
    "\n",
    "# # for c in data_spark.columns:\n",
    "# #     data_cent14 = data_cent14.withColumn(c+'*',col(c)*col('count') )\n",
    "# #     data_cent14 = data_cent14.drop(c).withColumnRenamed(c+'*', c)\n",
    "\n",
    "# # new_centroid = data_cent14.groupBy('defined_cluster').sum('first_numeric', 'second_numeric').\\\n",
    "# # withColumnRenamed('defined_cluster', 'defined_cluster*')\n",
    "\n",
    "# # new_centroid1 = data_cent14.groupBy('defined_cluster').sum('count')\n",
    "# # new_centroid = (new_centroid1.join(new_centroid, (col('defined_cluster') == col('defined_cluster*')),\\\n",
    "# #           \"inner\")).drop('defined_cluster*')\n",
    "# # ####\n",
    "# # for c in new_centroid.columns:\n",
    "# #     new_centroid = new_centroid.withColumnRenamed(c, c.replace('sum(', '').replace(')', ''))\n",
    "\n",
    "# # for c in new_centroid.columns[2:]:\n",
    "# #     new_centroid = new_centroid.withColumn(c, col(c) / col('sum(count)'))\n",
    "\n",
    "# # new_centroid = new_centroid.drop('sum(count)')\n",
    "# # ####\n",
    "# new_centroid = data_cent14.groupBy('defined_cluster').avg('first_numeric', 'second_numeric')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spark.sparkContext.getConf().getAll()\n",
    "# spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "# def UpdateCentroid(x):\n",
    "#     data_cent_join1 = data_spark.join(broadcast(x))\n",
    "#     data_cent_join2 = data_cent_join1.withColumn('dist',squaree_spark1(data_cent_join1.columns[0],\n",
    "#                                                                  data_cent_join1.columns[1],\n",
    "#                                        data_cent_join1.columns[3],data_cent_join1.columns[4]))#3 4\n",
    "#     w = Window.partitionBy(data_cent_join2.columns[1])\n",
    "#     next_centroid = data_cent_join2.withColumn('mindist', F.min('dist').over(w)).\\\n",
    "#     filter(col('dist') == col('mindist')).drop('dist')\n",
    "\n",
    "# #     #### Compute weighted average\n",
    "# #     data_cent14 = next_centroid.withColumn('index', row_number().\\\n",
    "# #                                               over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "# #     data_cent14 = (data_cent14.join(count, (col('Cindex') == col('index')),\\\n",
    "# #               \"inner\")).drop('index').drop('Cindex')\n",
    "\n",
    "# #     for c in data_spark.columns:\n",
    "# #         data_cent14 = data_cent14.withColumn(c+'*',col(c)*col('count') )\n",
    "# #         data_cent14 = data_cent14.drop(c).withColumnRenamed(c+'*', c)\n",
    "\n",
    "# #     new_centroid = data_cent14.groupBy('defined_cluster').sum('first_numeric', 'second_numeric').\\\n",
    "# #     withColumnRenamed('defined_cluster', 'defined_cluster*')\n",
    "\n",
    "# #     new_centroid1 = data_cent14.groupBy('defined_cluster').sum('count')\n",
    "# #     new_centroid = (new_centroid1.join(new_centroid, (col('defined_cluster') == col('defined_cluster*')),\\\n",
    "# #               \"inner\")).drop('defined_cluster*')\n",
    "\n",
    "\n",
    "# #     for c in new_centroid.columns[2:]:\n",
    "# #         new_centroid = new_centroid.withColumn(c, col(c) / col('sum(count)'))\n",
    "\n",
    "# #     update_new_centroid = new_centroid.drop('sum(count)')\n",
    "# #     ############\n",
    "#     update_new_centroid = next_centroid.groupBy('defined_cluster').avg('first_numeric', 'second_numeric')\n",
    "\n",
    "#     return update_new_centroid, next_centroid\n",
    "\n",
    "# new_centroid_persist = new_centroid.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# # start = timer()\n",
    "\n",
    "# for i in range(20):\n",
    "#     new_centroid_persist, final_data = UpdateCentroid(new_centroid_persist)\n",
    "\n",
    "# # end = timer()\n",
    "\n",
    "\n",
    "# final_data = final_data.withColumnRenamed('avg(first_numeric)','cent_x').\\\n",
    "# withColumnRenamed('avg(second_numeric)','cent_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# new_centroid_persist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(new_centroid_persist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_cent11 = sc.parallelize([])\n",
    "start = timer()\n",
    "\n",
    "spark_cluster_centroid = sqlContext.createDataFrame(([c.tolist() for c in clusters.centers]),\\\n",
    "                                                    ['cent_x', 'cent_y'])\n",
    "spark_cluster_centroid = spark_cluster_centroid.withColumn('defined_cluster', row_number().\\\n",
    "                                          over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "data_cent11 = 0\n",
    "for i in range(k):\n",
    "    u = [ str(i)+'x',str(i)+'y']\n",
    "    next_selected = spark_cluster_centroid.filter(col('defined_cluster') == str(i)).\\\n",
    "    drop('defined_cluster').toDF(*u)\n",
    "    if i == 0:\n",
    "        data_cent11 = data_spark_df.join(broadcast(next_selected))\n",
    "#         print(data_cent.count())\n",
    "        \n",
    "        data_cent11 = data_cent11.withColumn(str(i),squaree_spark1(data_cent11.columns[0],\\\n",
    "                            data_cent11.columns[1],data_cent11.columns[i+2],data_cent11.columns[i+3]))\n",
    "#         data_cent11.show()\n",
    "        data_cent11 = data_cent11.drop(data_cent11.columns[i+2]).drop(data_cent11.columns[i+3])\n",
    "#         data_cent11.show()\n",
    "        data_cent11 = data_cent11.withColumn('mindist',col(str(i)))\n",
    "#         data_cent11.show()\n",
    "        data_cent11 = data_cent11.withColumn('mindist1',least(data_cent11.columns[i+2], col('mindist')))\n",
    "#         data_cent11.show()\n",
    "        data_cent11 = data_cent11.drop('mindist')\n",
    "        data_cent11 = data_cent11.withColumnRenamed('mindist1','mindist')\n",
    "    elif i > 0:\n",
    "        data_cent11 = data_cent11.join(broadcast(next_selected))\n",
    "#         data_cent11.show()\n",
    "        data_cent11 = data_cent11.withColumn(str(i),squaree_spark1(data_cent11.columns[0],\\\n",
    "                                    data_cent11.columns[1], data_cent11.columns[i+3],data_cent11.columns[i+4]))\n",
    "#         data_cent11.show()\n",
    "        data_cent11 = data_cent11.drop(u[0]).drop(u[1])\n",
    "#         data_cent11.show()\n",
    "        data_cent11 = data_cent11.withColumn('mindist1',least(data_cent11.columns[i+3], col('mindist')))#4\n",
    "#         data_cent11.show()\n",
    "        data_cent11 = data_cent11.drop('mindist')\n",
    "        data_cent11 = data_cent11.withColumnRenamed('mindist1','mindist')\n",
    "#         next_cent = data_cent11.orderBy(desc('mindist')).limit(1).select(data_cent11.\\\n",
    "#                                                                          columns[:len(data_spark_df.columns)-1])#1:3\n",
    "data_cent11 = data_cent11.drop('mindist')\n",
    "\n",
    "# data_cent11.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindMinCOl( *row_list):\n",
    "    ind = row_list.index(min(*row_list))\n",
    "    return int(ind)\n",
    "find_min_val_name = udf(FindMinCOl, IntegerType())\n",
    "\n",
    "data_cent11 = data_cent11.withColumn('defined_cluster', find_min_val_name(*data_cent11.columns[2:3+k]))\n",
    "data_cent11 = data_cent11.select('first','second','defined_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data_cent11.toPandas()\n",
    "\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data = new_centroid_persist.withColumnRenamed('avg(first_numeric)','cent_x').\\\n",
    "# withColumnRenamed('avg(second_numeric)','cent_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data = fspark_cluster_centroid.withColumn('index', row_number().\\\n",
    "#                                           over(Window.orderBy(monotonically_increasing_id())) - 1).\\\n",
    "# drop('mindist').drop('second_numeric').drop('first_numeric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaders = leaders.withColumn('index_column_name', row_number().\\\n",
    "#                                           over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "# # final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaders1 = leaders.drop('count')\n",
    "# leaders1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lead_final = (leaders1\n",
    "#     .join(final_data, (col('index_column_name') == col('index')), \"inner\")).drop('1').drop('2').\\\n",
    "#     drop('index_column_name').drop('index')\n",
    "# #     .withColumnRenamed(\"defined_cluster\", \"new label\"))\n",
    "# lead_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lead_final = lead_final.withColumnRenamed('chunkLabel','chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_lead_final = (labels\n",
    "#     .join(lead_final,  ((col('label') == col('old label')) & (col('chunkLabel') == col('chunk'))),\\\n",
    "#           \"left\")).drop('chunkLabel').drop('label').drop('old label').drop('chunk')\n",
    "\n",
    "# # label_lead_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = label_lead_final.toPandas()\n",
    "# d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primariy Data: Max( o(n \\* p), o(n \\* n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# means:  uper(n / chunks) \\* k \\* p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 * 1) > 1 / chunks * k * 1 => chunks > k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# labels: n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_spark_df.where(col(\"index_column_name\").between(0, chunk-1)).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = list(range(data_spark_df.count()))\n",
    "# # data_spark_df.withColumn('index', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = timer()\n",
    "\n",
    "# labelsPred = NPIR(x,k,IR,i)\n",
    "# # labelsPred1 = NPIR(x1,k,IR,i)\n",
    "# end = timer()\n",
    "# print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))\n",
    "# labelsPred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xf = pd.DataFrame(x)\n",
    "# xf['label'] = labelsPred\n",
    "\n",
    "# xf1 = pd.DataFrame(x1)\n",
    "# xf1['label'] = labelsPred1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cs(labelsPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leader = []\n",
    "# f = list(Cs(labelsPred))\n",
    "# f.sort()\n",
    "# for i in f:\n",
    "#     leader.append([np.mean(z) for z in xf[xf['label']==i].values[:,:-1].T])\n",
    "# leader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leader1 = []\n",
    "# f = list(Cs(labelsPred1))\n",
    "# f.sort()\n",
    "# for i in f:\n",
    "#     leader1.append([np.mean(z) for z in xf1[xf1['label']==i].values[:,:-2].T])\n",
    "# leader1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xf1[xf1['label']==0].values[:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 2 3 4 5 6 7 \n",
    "# 1 2 3 4\n",
    "# 1 2\n",
    "# 1\n",
    "\n",
    "# 2**b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a \\* t(NPIR(n / a)) + (a - 1) \\* t(NPIR(k)) ><? t(NPIR(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centers = leader.copy()\n",
    "# centers.extend(leader1)\n",
    "# centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newlabel = NPIR(centers,k,IR,i)\n",
    "# newlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newlabel[:k][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newleader =[]\n",
    "# for i in range(k):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xf['newlabel'] = np.full(len(xf), np.nan)\n",
    "# xf1['newlabel'] =np.full(len(xf1), np.nan)\n",
    "# # f = list(Cs(labelsPred))\n",
    "# # f.sort()\n",
    "# for i in range(k):\n",
    "#     xf.loc[xf['label'] == i,'newlabel'] = newlabel[:k][i]\n",
    "\n",
    "# # f = list(Cs(labelsPred1))\n",
    "# # f.sort()\n",
    "# for i in range(k):\n",
    "#     xf1.loc[xf1['label'] == i,'newlabel'] = newlabel[-k:][i]\n",
    "# merged_data = xf.append(xf1)\n",
    "# merged_data['label'] = merged_data['newlabel']\n",
    "# merged_data.drop(['newlabel'], inplace =True,axis=1)\n",
    "# merged_data.reset_index(inplace=True)\n",
    "# merged_data.drop(['index'], inplace =True,axis=1)\n",
    "# merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xf = merged_data.copy()\n",
    "# labelsPred = merged_data['label']\n",
    "# xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tStart = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('blobs2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['0','1','label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = d1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['new'] = d['defined_cluster'].astype('int8')\n",
    "# data['new'] = NPIR(pd.read_csv('blobs.csv', header=None).iloc[:,:2].values,k,IR,i)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['label']\n",
    "labelsPred = data['new']\n",
    "# list(labelsPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[['0','1']].values\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing results\n",
    "print('labels:')\n",
    "# print(labelsPred)\n",
    "\n",
    "# tEnd = datetime.datetime.now()\n",
    "# print('Time: ' + str(tEnd - tStart))\n",
    "print('Measures:')\n",
    "print('HS: ' + str(metrics.homogeneity_score(y,labelsPred)))\n",
    "print('CS: ' + str(metrics.completeness_score(y,labelsPred)))\n",
    "print('VM: ' + str(metrics.v_measure_score(y,labelsPred)))\n",
    "print('AMI: ' + str(metrics.adjusted_mutual_info_score(y,labelsPred)))\n",
    "print('ARI: ' + str(metrics.adjusted_rand_score(y,labelsPred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle, islice \n",
    "\n",
    "fig = plt.figure()      \n",
    "colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a','#f781bf', '#a65628', '#984ea3',\n",
    "                                    '#999999', '#e41a1c', '#dede00']),int(k))))\n",
    "plt.scatter(x[:, 0], x[:, 1], s=10, color=colors[labelsPred.tolist()])\n",
    "plt.show()\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from itertools import cycle, islice \n",
    "\n",
    "# fig = plt.figure()      \n",
    "# colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a','#f781bf', '#a65628', '#984ea3',\n",
    "#                                     '#999999', '#e41a1c', '#dede00']),int(k))))\n",
    "# plt.scatter(x1[:, 0], x1[:, 1], s=10, color=colors[labelsPred1.tolist()])\n",
    "# plt.show()\n",
    "# # plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from itertools import cycle, islice \n",
    "\n",
    "# fig = plt.figure()      \n",
    "# colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a','#f781bf', '#a65628', '#984ea3',\n",
    "#                                     '#999999', '#e41a1c', '#dede00']),int(k))))\n",
    "# plt.scatter(merged_data.iloc[:, 0].values, merged_data.iloc[:, 1].values, s=10, color=colors[merged_data['label'].astype(int).tolist()])\n",
    "# plt.show()\n",
    "# # plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
