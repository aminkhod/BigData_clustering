{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from NPIR import NPIR\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# ### multiprocessing\n",
    "# from multiprocessing.pool import Pool\n",
    "# import multiprocessing\n",
    "\n",
    "\n",
    "import datetime\n",
    "# import warnings\n",
    "from collections import Counter as Cs\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from operator import *\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "# from pyspark import StorageLevel\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, log, rand, col, broadcast, row_number, avg, mean, least, struct,\\\n",
    "                lit, sequence, sum, monotonically_increasing_id, pandas_udf, PandasUDFType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession, SQLContext, Window, Row, DataFrame\n",
    "from pyspark import SparkConf\n",
    "from scipy.spatial import distance\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").\\\n",
    "config(\"spark.storage.blockManagerSlaveTimeoutMs\",\"12000001ms\").config(\"spark.driver.maxResultSize\",\"24g\").\\\n",
    "config(\"spark.default.parallelism\", \"200\").config(\"spark.memory.offHeap.size\", \"24g\").\\\n",
    "appName(\"NPIR_Parallel\").config(\"spark.executor.memory\", \"24g\").config(\"spark.driver.memory\", \"24g\").\\\n",
    "getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"False\")\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NPIRPreProcess(points, chunk):\n",
    "    \n",
    "    #IR: The indexing ratio to be used for generating the maximum index\n",
    "    IR = 0.2\n",
    "    #The number of iteration i\n",
    "    i = 10\n",
    "    k = 3 #k: Number of clusters\n",
    "    # count = Cs()\n",
    "#     leaderheadr = ['chunkLabel', 'old label']\n",
    "#     # leaderheadr = []\n",
    "#     leaderheadr.extend([str(x) for x in range(1, len(data_spark.columns))])\n",
    "#     leaderheadr = tuple(leaderheadr)\n",
    "\n",
    "    start = timer()\n",
    "    # labels = sqlContext.createDataFrame([np.full(len(labelsheader), np.nan).tolist()],labelsheader)\n",
    "    # labels = labels.na.drop()\n",
    "\n",
    "#     leaders = sqlContext.createDataFrame([np.full(len(leaderheadr), np.nan).tolist()],leaderheadr)\n",
    "#     leaders = leaders.na.drop()\n",
    "\n",
    "#     ii = 0\n",
    "    print(SparkFiles.get('blobs1.csv'))\n",
    "    data_spark_df = spark.read.format('csv').option('header','True').option('index','False').\\\n",
    "    load(SparkFiles.get(some_path))\n",
    "    for z in range(0, points, chunk):\n",
    "        j = z + chunk\n",
    "\n",
    "        data = data_spark_df.where(col(\"index_column_name\").between(z, min(points, j-1))).toPandas()\n",
    "        data.drop(\"index_column_name\",axis=1,inplace=True)\n",
    "        data = data.astype(float)\n",
    "        from NPIR import NPIR\n",
    "        label = NPIR(data.values,k,IR,i)\n",
    "\n",
    "        del NPIR\n",
    "        data['labels'] = label\n",
    "\n",
    "    #     # Adding to pyspard label\n",
    "    #     chunklabel = np.full(len(label), ii).tolist()\n",
    "    #     labelDF = [(x, y) for x, y in zip(chunklabel, label)]\n",
    "    #     labelsDF = sqlContext.createDataFrame(labelDF,labelsheader)\n",
    "    #     labels = unionAll(labels, labelsDF)\n",
    "\n",
    "        leader = []\n",
    "        f = list(Cs(label))\n",
    "        f.sort()\n",
    "        for i in f:\n",
    "            leader.append([round(np.mean(z), 4) for z in data[data['labels']==i].values[:, :-1].T])\n",
    "        del data\n",
    "\n",
    "#         # Adding to pyspark leaders\n",
    "#         for x in range(len(leader)):\n",
    "#             x1 = [ii, x]\n",
    "#             x1.extend(leader[x])\n",
    "#             leader[x] = x1\n",
    "#         leaderDF = sqlContext.createDataFrame(leader,leaderheadr)\n",
    "#         leaders = unionAll(leaders, leaderDF)\n",
    "#         ii += 1\n",
    "#     del data_spark\n",
    "    return leader\n",
    "    end = timer()\n",
    "    print (\"Execution time HH:MM:SS:\", timedelta(seconds= end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
