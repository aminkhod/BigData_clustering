{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from NPIR import NPIR\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# ### multiprocessing\n",
    "# from multiprocessing.pool import Pool\n",
    "# import multiprocessing\n",
    "\n",
    "\n",
    "import datetime\n",
    "# import warnings\n",
    "from collections import Counter as Cs\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkFiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-c20c969d6323>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-c20c969d6323>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def class NPIRworker:\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def class NPIRworker:\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        self.points = points\n",
    "        #IR: The indexing ratio to be used for generating the maximum index\n",
    "        self.IR = 0.2\n",
    "        #The number of iteration i\n",
    "        self.i = 10\n",
    "        self.k = 3 #k: Number of clusters\n",
    "        # count = Cs()\n",
    "        self.chunk = 200\n",
    "    def NPIRPreProcess():\n",
    "        leaderheadr = ['chunkLabel', 'old label']\n",
    "        # leaderheadr = []\n",
    "        leaderheadr.extend([str(x) for x in range(1, len(data_spark.columns))])\n",
    "        leaderheadr = tuple(leaderheadr)\n",
    "        \n",
    "        start = timer()\n",
    "        # labels = sqlContext.createDataFrame([np.full(len(labelsheader), np.nan).tolist()],labelsheader)\n",
    "        # labels = labels.na.drop()\n",
    "\n",
    "        leaders = sqlContext.createDataFrame([np.full(len(leaderheadr), np.nan).tolist()],leaderheadr)\n",
    "        leaders = leaders.na.drop()\n",
    "\n",
    "        ii = 0\n",
    "        for z in range(0, points, chunk):\n",
    "            j = z + chunk\n",
    "            data = data_spark.where(col(\"index_column_name\").between(z, j-1)).toPandas()\n",
    "            data.drop(\"index_column_name\",axis=1,inplace=True)\n",
    "            data = data.astype(float)\n",
    "            from NPIR import NPIR\n",
    "            label = NPIR(data.values,k,IR,i)\n",
    "\n",
    "            del NPIR\n",
    "            data['labels'] = label\n",
    "\n",
    "        #     # Adding to pyspard label\n",
    "        #     chunklabel = np.full(len(label), ii).tolist()\n",
    "        #     labelDF = [(x, y) for x, y in zip(chunklabel, label)]\n",
    "        #     labelsDF = sqlContext.createDataFrame(labelDF,labelsheader)\n",
    "        #     labels = unionAll(labels, labelsDF)\n",
    "\n",
    "            leader = []\n",
    "            f = list(Cs(label))\n",
    "            f.sort()\n",
    "            for i in f:\n",
    "                leader.append([round(np.mean(z), 4) for z in data[data['labels']==i].values[:, :-1].T])\n",
    "            del data\n",
    "\n",
    "            # Adding to pyspark leaders\n",
    "            for x in range(len(leader)):\n",
    "                x1 = [ii, x]\n",
    "                x1.extend(leader[x])\n",
    "                leader[x] = x1\n",
    "            leaderDF = sqlContext.createDataFrame(leader,leaderheadr)\n",
    "            leaders = unionAll(leaders, leaderDF)\n",
    "            ii += 1\n",
    "        del data_spark\n",
    "        end = timer()\n",
    "        print (\"Execution time HH:MM:SS:\", timedelta(seconds= end - start))\n",
    "        start = timer()\n",
    "        # parsedData = leaders.select(['1', '2']).rdd.map(list)\n",
    "        leaders.toPandas().to_csv('leaders.csv')\n",
    "        del leaders\n",
    "        leaders_spark_df = spark.read.format('csv').option('header','True').option('index','False').load('leaders.csv')\n",
    "        parsedData = leaders_spark_df.select(leaders_spark_df.columns[2:]).rdd.map(list)\n",
    "        del leaders_spark_df\n",
    "\n",
    "        end = timer()\n",
    "        print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
